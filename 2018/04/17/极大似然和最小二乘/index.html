<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><title>极大似然和最小二乘 | Machine Learning</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="机器学习,深度学习,算法"><meta name="description" content=""><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="http://yoursite.com/2018/04/17/极大似然和最小二乘/index.html"><link rel="icon" type="image/png" href="http://oo12ugek5.bkt.clouddn.com/blog/images/favicon.ico" sizes="32x32"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="dalalaa读书日记"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-116821310-1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-116821310-1")</script><link rel="stylesheet" href="/scss/views/page/post.css"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(http://oo12ugek5.bkt.clouddn.com/blog/images/loader.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="dalalaa读书日记" alt="dalalaa读书日记"><img src="undefined" alt="dalalaa读书日记"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li><li class="nav__item"><a href="/search" alt="menu.search" title="menu.search">menu.search</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="http://oo12ugek5.bkt.clouddn.com/images/default_cover.png" alt="极大似然和最小二乘"></div><header class="post__info"><h1 class="post__title">极大似然和最小二乘</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/Arctanxy">dalalaa</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2018-04-17</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/机器学习/">机器学习</a></li><li class="mark__item"><a href="/tags/线性回归/">线性回归</a></li></ul></div></div></header><div class="post__content"><h1 id="极大似然和最小二乘"><a href="#极大似然和最小二乘" class="headerlink" title="极大似然和最小二乘"></a>极大似然和最小二乘</h1><p>本文为PRML第三章3.1.1笔记，书中推导步骤过于简单，对长期没有接触高数的读者很不友好，故在此记录一下由极大似然到最小二乘的详细的推导过程。</p><p>线性回归的方程可以写成如下形式：</p><script type="math/tex;mode=display">y = f(x,w) + \epsilon</script><p>其中$\epsilon$是一个均值为零的高斯随机变量，精度（方差的倒数）为$\beta$。</p><p>可以认为$y$值符合均值为$f(x,w)$，方差为$\beta^{-1}$的高斯分布：</p><script type="math/tex;mode=display">p(y|x,w,\beta) = \mathcal N(y|f(x,w),\beta^{-1})</script><p>根据高斯分布的概率分布公式可以转化为：</p><script type="math/tex;mode=display">f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}

 = \sqrt{\frac{\beta}{2\pi}} e^{-\frac{\beta(x-\mu)^2}{2}}</script><script type="math/tex;mode=display">\mathcal N(y|f(x,w),\beta^{-1}) = \sqrt{\frac{\beta}{2\pi}} e^{-\frac{\beta(y-f(x,w))^2}{2}}</script><p>在此条件分布下，可以很快地求得$y$的条件均值：</p><script type="math/tex;mode=display">E[y|x] = \int yp(y|x)dy = f(x,w)</script><p>在实际问题中，$X$为数据集${x_1,…,x_n}$，对应的目标值$y_1,…y_n$，${y_n}$是一个列向量，记作$y$，可以得到如下公式：</p><script type="math/tex;mode=display">p(y|X,w,\beta) = \prod_{n=1}^{N} \mathcal N(y_n|w^T\phi(x_n),\beta^{-1})</script><p>对两边同时取导数</p><script type="math/tex;mode=display">ln p(y|X,w,\beta) = \sum_{n=1}^{N}ln \mathcal N(y_n|w^T\phi(x_n),\beta^{-1})</script><p>将前面的概率公式带入之后可以写成</p><script type="math/tex;mode=display">ln p(y|X,w,\beta) = \sum_{n=1}^{N}ln \{\sqrt{\frac{\beta}{2\pi}} e^{-\frac{\beta(y-f(x,w))^2}{2}}\}</script><script type="math/tex;mode=display">ln p(y|X,w,\beta) = \sum_{n=1}^{N} \{\frac{ln \beta}{2} - \frac{ln2\pi}{2} - \frac{\beta (y_n-f(x,w))^2}{2}\}</script><script type="math/tex;mode=display">ln p(y|X,w,\beta) = \frac{Nln \beta}{2} - \frac{Nln2\pi}{2} -\sum_{n=1}^{N} \frac{\beta (y_n-f(x,w))^2}{2}</script><p>为了使概率$p(y|X,w,\beta)$最大，就需要使$\sum<em>{n=1}^{N} \frac{\beta (y_n-f(x,w))^2}{2}$最小，也就是使$\sum</em>{n=1}^{N} (y_n-f(x,w))^2$最小。</p><p>如此一来，又变成了一个最小二乘问题，对其求导后令导数为零，即可得到最终$w$的解(具体推导过程参考<a href="https://arctanxy.github.io/2018/04/05/%E5%B2%AD%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">线性回归与岭回归的数学推导</a>)：</p><script type="math/tex;mode=display">w = (X^TX)^{-1}y^T X</script><div class="post-announce">感谢您的阅读，本文由 <a href="http://yoursite.com">dalalaa读书日记</a> 版权所有。如若转载，请注明出处：dalalaa读书日记（<a href="http://yoursite.com/2018/04/17/极大似然和最小二乘/">http://yoursite.com/2018/04/17/极大似然和最小二乘/</a>）</div><div class="post__prevs"><div class="post__prev"><a href="/2018/04/15/LSTM预测汇率变化/" title="使用循环神经网络预测汇率涨跌"><i class="iconfont icon-prev"></i>使用循环神经网络预测汇率涨跌</a></div><div class="post__prev post__prev--right"><a href="/2018/04/17/朴素贝叶斯分类器的Python实现/" title="朴素贝叶斯分类器的Python实现">朴素贝叶斯分类器的Python实现<i class="iconfont icon-next"></i></a></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">文章分类</h3></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2018/05/20/回归问题中的小套路/" title="回归问题中的小套路"><div class="item__cover"><img src="/img/houseprice/house.jpg" alt="回归问题中的小套路"></div><div class="item__info"><h3 class="item__title">回归问题中的小套路</h3><span class="item__text">2018-05-20</span></div></a></li><li class="latest-post-item"><a href="/2018/04/30/OpenCV简单用法（一）/" title="OpenCV简单用法（一）"><div class="item__cover"><img src="/img/OpenCV简单用法/Diablo.jpg" alt="OpenCV简单用法（一）"></div><div class="item__info"><h3 class="item__title">OpenCV简单用法（一）</h3><span class="item__text">2018-04-30</span></div></a></li><li class="latest-post-item"><a href="/2018/04/30/OpenCV简单用法（二）/" title="OpenCV简单用法（二）"><div class="item__cover"><img src="/img/OpenCV简单用法/Atreus.jpg" alt="OpenCV简单用法（二）"></div><div class="item__info"><h3 class="item__title">OpenCV简单用法（二）</h3><span class="item__text">2018-04-30</span></div></a></li><li class="latest-post-item"><a href="/2018/04/20/LSTM预测股票走势/" title="LSTM预测股票走势"><div class="item__cover"><img src="https://github.com/Arctanxy/learning_notes/blob/master/study/ttfunds/prediction.jpg?raw=true" alt="LSTM预测股票走势"></div><div class="item__info"><h3 class="item__title">LSTM预测股票走势</h3><span class="item__text">2018-04-20</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/LSTM/">LSTM</a></li><li class="tag-item"><a class="tag-link" href="/tags/OpenCV/">OpenCV</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-item"><a class="tag-link" href="/tags/回归/">回归</a></li><li class="tag-item"><a class="tag-link" href="/tags/图像处理/">图像处理</a></li><li class="tag-item"><a class="tag-link" href="/tags/时间序列/">时间序列</a></li><li class="tag-item"><a class="tag-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-item"><a class="tag-link" href="/tags/深度学习/">深度学习</a></li><li class="tag-item"><a class="tag-link" href="/tags/特征工程/">特征工程</a></li><li class="tag-item"><a class="tag-link" href="/tags/线性回归/">线性回归</a></li><li class="tag-item"><a class="tag-link" href="/tags/贝叶斯/">贝叶斯</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text"></p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span></span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span></span></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>.</p><ul class="footer__social-network clearfix"></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>