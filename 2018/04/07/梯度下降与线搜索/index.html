<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><title>线性回归中的梯度下降与一维搜索 |</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="dalalaa"><meta name="designer" content="minfive"><meta name="keywords" content="null"><meta name="description" content="Machine Learning"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="http://yoursite.com/2018/04/07/梯度下降与线搜索/index.html"><link rel="icon" type="image/png" href="undefined" sizes="32x32"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="阿呆的读书笔记"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-116821310-1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-116821310-1")</script><link rel="stylesheet" href="/scss/views/page/post.css"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(undefined)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="阿呆的读书笔记" alt="阿呆的读书笔记"><img src="undefined" alt="阿呆的读书笔记"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li><li class="nav__item"><a href="/search" alt="menu.search" title="menu.search">menu.search</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="undefined" alt="线性回归中的梯度下降与一维搜索"></div><header class="post__info"><h1 class="post__title">线性回归中的梯度下降与一维搜索</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/">undefined</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2018-04-07</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/回归/">回归</a></li><li class="mark__item"><a href="/tags/机器学习/">机器学习</a></li></ul></div></div></header><div class="post__content"><h1 id="线性回归中的梯度下降与一维搜索"><a href="#线性回归中的梯度下降与一维搜索" class="headerlink" title="线性回归中的梯度下降与一维搜索"></a>线性回归中的梯度下降与一维搜索</h1><p>之前讲到了一般线性回归和岭回归的矩阵求解方式，但是并非所有的模型都能方便地求出数学最优解，往往需要采取逐步迭代的方式寻找近似最优解。常见的方法有梯度下降法、牛顿迭代法、拟牛顿法等。为了便于直观地展现求解过程，本文中将以求二次函数的最小值为例，讲解梯度下降法的使用方法。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>假设有函数$y = 2x^2+ 3x +4$，绘制出的函数图像如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = np.linspace(<span class="number">-22</span>,<span class="number">20</span>,<span class="number">100</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = <span class="number">2</span>*x**<span class="number">2</span> + <span class="number">3</span>*x + <span class="number">4</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line">y = func(x)</span><br><span class="line">plt.plot(x,y,color = <span class="string">'g'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x86b9a90&gt;]
</code></pre><p><img src="http://Arctanxy.github.io/article_fig/output_3_1.png" alt="png"></p><p>可以很容易的得到y的导数方程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span>*x + <span class="number">3</span></span><br></pre></td></tr></table></figure><p>给定任意初始值x_0，设置步长为0.2，根据梯度下降原理，对最优解进行搜索</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimize</span><span class="params">(x0 = <span class="number">10</span>,step = <span class="number">0.2</span>)</span>:</span></span><br><span class="line">    x = x0</span><br><span class="line">    path = []</span><br><span class="line">    path.append(x)</span><br><span class="line">    <span class="comment">#先迭代五次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        x = x - step*gradient(x)</span><br><span class="line">        print(x)</span><br><span class="line">        path.append(x)</span><br><span class="line">    <span class="keyword">return</span> path</span><br><span class="line">path = minimize()</span><br><span class="line">plt.plot(x,y,color = <span class="string">'g'</span>)</span><br><span class="line">plt.plot(path,[func(x) <span class="keyword">for</span> x <span class="keyword">in</span> path],color = <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>1.4000000000000004
-0.32000000000000006
-0.664
-0.7328
-0.74656
-0.749312
-0.7498624
-0.74997248
-0.749994496
-0.7499988992





[&lt;matplotlib.lines.Line2D at 0x89bc278&gt;]
</code></pre><p><img src="http://Arctanxy.github.io/article_fig/output_7_2.png" alt="png"></p><p>如果换一个x0效果会如何呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">path = minimize(<span class="number">20</span>)</span><br><span class="line">plt.plot(x,y,color = <span class="string">'g'</span>)</span><br><span class="line">plt.plot(path,[func(x) <span class="keyword">for</span> x <span class="keyword">in</span> path],color = <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>3.3999999999999986
0.07999999999999963
-0.5840000000000001
-0.7168
-0.74336
-0.748672
-0.7497344
-0.74994688
-0.749989376
-0.7499978752





[&lt;matplotlib.lines.Line2D at 0x568a2b0&gt;]
</code></pre><p><img src="http://Arctanxy.github.io/article_fig/output_9_2.png" alt="png"></p><p>其实对于这种单峰函数来说，初始值对最后结果的影响并不大，只是会影响迭代的次数而已。</p><p>我们再换增大一点步长试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">path = minimize(<span class="number">10</span>,<span class="number">0.6</span>)</span><br><span class="line">x = np.linspace(min(path),max(path),<span class="number">100</span>)</span><br><span class="line">plt.plot(x,[func(a) <span class="keyword">for</span> a <span class="keyword">in</span> x],color = <span class="string">'g'</span>)</span><br><span class="line">plt.plot(path,[func(b) <span class="keyword">for</span> b <span class="keyword">in</span> path],color = <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>-15.8
20.319999999999997
-30.247999999999994
40.54719999999999
-58.566079999999985
80.19251199999998
-114.06951679999996
157.89732351999993
-222.8562529279999
310.19875409919985





[&lt;matplotlib.lines.Line2D at 0x8b9a1d0&gt;]
</code></pre><p><img src="http://Arctanxy.github.io/article_fig/output_11_2.png" alt="png"></p><p>从图中可以看到，在步长设置不合理情况下，x有可能会逐渐偏离函数最小值，最终得到的函数值的绝对值越来越大，很快便会超出实数范围，得到inf。</p><p>为了解决这个问题，需要动态调整步长。</p><p>比较简单的思路是这样的：在每一步迭代之前检查一下y值是不是在增大，如果y值增大了，就缩小step的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimize</span><span class="params">(x0 = <span class="number">10</span>,step = <span class="number">0.2</span>,beta = <span class="number">0.8</span>)</span>:</span></span><br><span class="line">    x = x0</span><br><span class="line">    path = []</span><br><span class="line">    path.append(x)</span><br><span class="line">    <span class="comment">#先迭代五次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        new_x = x - step*gradient(x)</span><br><span class="line">        <span class="keyword">while</span> func(x) &lt; func(new_x):</span><br><span class="line">            step *= beta</span><br><span class="line">            new_x = x-step*gradient(x)</span><br><span class="line">        x = new_x</span><br><span class="line">        path.append(x)</span><br><span class="line">    <span class="keyword">return</span> path</span><br><span class="line">path = minimize(<span class="number">10</span>,<span class="number">0.6</span>)</span><br><span class="line">m = np.linspace(min(path),max(path),<span class="number">100</span>)</span><br><span class="line">plt.plot(m,[func(a) <span class="keyword">for</span> a <span class="keyword">in</span> m],color = <span class="string">'g'</span>)</span><br><span class="line">plt.plot(path,[func(b) <span class="keyword">for</span> b <span class="keyword">in</span> path],color = <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x8bd4da0&gt;]
</code></pre><p><img src="http://Arctanxy.github.io/article_fig/output_14_1.png" alt="png"></p><p>小小的改动之后就可以避免函数增大，如果希望收敛速度更快一些的话，就需要调节beta，调节beta的难度比调节step简单得多。</p><p>如果想加快收敛速度的话，可以改用更加严谨的一维搜索方法。</p><h2 id="一维搜索"><a href="#一维搜索" class="headerlink" title="一维搜索"></a>一维搜索</h2><p>在同样的beta条件下，一维搜索能够更快地收敛，并且能step变得太小。</p><p>一维搜索有很多种实现方式，上面用的这种算是效率较高的一维搜索算法，名为：回溯线搜索 backtracking line search。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">line_search</span><span class="params">(x,gradient,step,threshold = <span class="number">0.1</span>,beta = <span class="number">0.8</span>)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> func(x-step*gradient) &gt; func(x) - threshold * step * gradient**<span class="number">2</span>:</span><br><span class="line">        step *= beta</span><br><span class="line">    <span class="keyword">return</span> step</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">line_minimize</span><span class="params">(x0 = <span class="number">10</span>,step = <span class="number">0.2</span>,beta = <span class="number">0.8</span>)</span>:</span></span><br><span class="line">    x = x0</span><br><span class="line">    path = []</span><br><span class="line">    path.append(x)</span><br><span class="line">    <span class="comment">#先迭代五次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        step = line_search(x,gradient(x),step)</span><br><span class="line">        x = x - step * gradient(x)</span><br><span class="line">        path.append(x)</span><br><span class="line">    <span class="keyword">return</span> path</span><br><span class="line"></span><br><span class="line">path = line_minimize(<span class="number">10</span>,<span class="number">0.6</span>)</span><br><span class="line">m = np.linspace(min(path),max(path),<span class="number">100</span>)</span><br><span class="line">plt.plot(m,[func(a) <span class="keyword">for</span> a <span class="keyword">in</span> m],color = <span class="string">'g'</span>)</span><br><span class="line">plt.plot(path,[func(b) <span class="keyword">for</span> b <span class="keyword">in</span> path],color = <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x8c33d68&gt;]
</code></pre><p><img src="http://Arctanxy.github.io/article_fig/output_18_1.png" alt="png"></p><div class="post-announce">感谢您的阅读，本文由 <a href="http://yoursite.com">阿呆的读书笔记</a> 版权所有。如若转载，请注明出处：阿呆的读书笔记（<a href="http://yoursite.com/2018/04/07/梯度下降与线搜索/">http://yoursite.com/2018/04/07/梯度下降与线搜索/</a>）</div><div class="post__prevs"><div class="post__prev"><a href="/2018/04/05/Ridge/" title="线性回归的矩阵运算"><i class="iconfont icon-prev"></i>线性回归的矩阵运算</a></div><div class="post__prev post__prev--right"><a href="/2018/04/08/Random Lasso/" title="Random Lasso">Random Lasso<i class="iconfont icon-next"></i></a></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">Machine Learning</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2018/04/20/LSTM预测股票走势/" title="LSTM预测股票走势"><div class="item__cover"><img src="undefined" alt="LSTM预测股票走势"></div><div class="item__info"><h3 class="item__title">LSTM预测股票走势</h3><span class="item__text">2018-04-20</span></div></a></li><li class="latest-post-item"><a href="/2018/04/17/极大似然和最小二乘/" title="极大似然和最小二乘"><div class="item__cover"><img src="undefined" alt="极大似然和最小二乘"></div><div class="item__info"><h3 class="item__title">极大似然和最小二乘</h3><span class="item__text">2018-04-17</span></div></a></li><li class="latest-post-item"><a href="/2018/04/17/朴素贝叶斯分类器的Python实现/" title="朴素贝叶斯分类器的Python实现"><div class="item__cover"><img src="undefined" alt="朴素贝叶斯分类器的Python实现"></div><div class="item__info"><h3 class="item__title">朴素贝叶斯分类器的Python实现</h3><span class="item__text">2018-04-17</span></div></a></li><li class="latest-post-item"><a href="/2018/04/15/LSTM预测汇率变化/" title="使用循环神经网络预测汇率涨跌"><div class="item__cover"><img src="undefined" alt="使用循环神经网络预测汇率涨跌"></div><div class="item__info"><h3 class="item__title">使用循环神经网络预测汇率涨跌</h3><span class="item__text">2018-04-15</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/LSTM/">LSTM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-item"><a class="tag-link" href="/tags/回归/">回归</a></li><li class="tag-item"><a class="tag-link" href="/tags/时间序列/">时间序列</a></li><li class="tag-item"><a class="tag-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-item"><a class="tag-link" href="/tags/深度学习/">深度学习</a></li><li class="tag-item"><a class="tag-link" href="/tags/线性回归/">线性回归</a></li><li class="tag-item"><a class="tag-link" href="/tags/贝叶斯/">贝叶斯</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text"></p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span></span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span></span></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>.</p><ul class="footer__social-network clearfix"></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>