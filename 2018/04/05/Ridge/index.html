<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><title>线性回归的矩阵运算 |</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="dalalaa"><meta name="designer" content="minfive"><meta name="keywords" content="null"><meta name="description" content="Machine Learning"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="http://yoursite.com/2018/04/05/Ridge/index.html"><link rel="icon" type="image/png" href="undefined" sizes="32x32"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="阿呆的读书笔记"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-116821310-1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-116821310-1")</script><link rel="stylesheet" href="/scss/views/page/post.css"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(undefined)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="阿呆的读书笔记" alt="阿呆的读书笔记"><img src="undefined" alt="阿呆的读书笔记"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li><li class="nav__item"><a href="/search" alt="menu.search" title="menu.search">menu.search</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="undefined" alt="线性回归的矩阵运算"></div><header class="post__info"><h1 class="post__title">线性回归的矩阵运算</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/">undefined</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2018-04-05</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"></ul></div></div></header><div class="post__content"><h1 id="岭回归的矩阵运算"><a href="#岭回归的矩阵运算" class="headerlink" title="岭回归的矩阵运算"></a>岭回归的矩阵运算</h1><h2 id="普通线性回归的矩阵运算"><a href="#普通线性回归的矩阵运算" class="headerlink" title="普通线性回归的矩阵运算"></a>普通线性回归的矩阵运算</h2><p>在训练数据<br>（1）各列数据线性独立<br>（2）样本数量多于特征数量<br>的前提下，可以使用矩阵形式计算线性回归的系数<br><a href="http://bourneli.github.io/linear-algebra/2016/03/03/linear-algebra-04-ATA-inverse.html" target="_blank" rel="noopener">参考</a></p><h3 id="想象中的推导过程"><a href="#想象中的推导过程" class="headerlink" title="想象中的推导过程"></a>想象中的推导过程</h3><p>$$ Xw = Y $$</p><p>$$ X^TXw = X^TY$$</p><p>$$ w = (X^TX)^{-1}X^TY$$</p><p>步骤很简单，但是其实这是一种野路子的算法，因为这一步的原始方程$Xw = Y$不是一个能直接使用的方程，这个方程可以通过坐标投影得到，只能作为一个经验公式使用，具体可以参见<a href="https://zhuanlan.zhihu.com/p/22757336。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22757336。</a></p><h3 id="真实的推导过程"><a href="#真实的推导过程" class="headerlink" title="真实的推导过程"></a>真实的推导过程</h3><p>推导之前需要了解一些关于矩阵范数的知识。</p><h3 id="矩阵范数"><a href="#矩阵范数" class="headerlink" title="矩阵范数"></a>矩阵范数</h3><p>矩阵的L2范数</p><p>$$||A||^2_2 = \lambda$$</p><p>其中$\lambda$是矩阵$A^TA$的最大特征值。</p><p>因此，对于一维矩阵（向量）$a$而言，其L2范数就是$a^Ta$，因为$a^Ta$是一个标量，所以$a^Ta$的值就是其最大特征值。</p><h3 id="矩阵微分"><a href="#矩阵微分" class="headerlink" title="矩阵微分"></a>矩阵微分</h3><p>当$X$，$\beta$是一维矩阵（向量）时，</p><p>$$ \frac{\partial \beta^TX}{X} = \beta$$</p><p>$$ \frac{\partial X^TX}{\partial X} = X$$</p><p>$$ \frac{X^TAX}{\partial X} = (A + A^T)X$$</p><p><a href="https://blog.csdn.net/u010976453/article/details/54381248" target="_blank" rel="noopener">矩阵微分公式</a></p><p>回归的目标是最小化</p><p>$$min(||Y-XW||^2_2)$$</p><h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><p>$$ F = ||Y-XW||^2_2 = (Y-XW)^T(Y-XW)$$</p><p>$$ F = (Y^T - W^TX^T)(Y-XW)$$</p><p>$$ F = Y^TY - Y^TXW - W^TX^TY + W^TX^TXW$$</p><p>对F进行求导得到：</p><p>$$ \frac{\partial F}{\partial W} = - Y^TX - Y^TX + (X^TX + X^TX)W $$</p><p>$$ \frac{\partial F}{\partial W} = -2 Y^TX + 2X^TXW $$</p><p>令$\frac{\partial F}{\partial W} = 0$，有：</p><p>$$\frac{\partial F}{\partial W} = -2 Y^TX + 2X^TXW = 0$$</p><p>$$X^TXW = Y^TX$$</p><p>$$W = (X^TX)^{-1} Y^TX$$</p><h3 id="利用矩阵的迹的性质推导"><a href="#利用矩阵的迹的性质推导" class="headerlink" title="利用矩阵的迹的性质推导"></a>利用矩阵的迹的性质推导</h3><p>参见斯坦福大学cs229课程笔记<br>依赖公式：</p><p>$$ tr(a) = a$$</p><p>$$ trAB = trBA$$</p><p>$$ trA = trA^T$$</p><p>$$ \nabla_AtrAB = B^T$$</p><p>$$ \nabla_{A^T} trABA^TC = B^TA^TC^T + BA^TC$$</p><p>其中a是一个实数</p><p>也可以写成</p><p>$$\frac{\partial(trAB)}{\partial A} = B^T$$</p><p>$$\frac{\partial(trABA^TC)}{A^T} = B^TA^TC^T + BA^TC$$</p><p>但是这个推导过程用梯度形式更好书写，所以下面还是遵照原笔记中的梯度公式进行推导</p><p>推导过程：</p><p>$$ F = ||Y-XW||^2_2 = (Y-XW)^T(Y-XW)$$</p><p>直接求导</p><p>$$ \nabla_W F(W) = \nabla_W(Y-XW)^T(Y-XW) $$</p><p>$$ = \nabla_W(Y^TY - Y^TXW - W^TX^TY + W^TX^TXW)$$</p><p>因为括号中的所有项的计算结果是一个实数，所以：</p><p>$$ \nabla_W F(W) = \nabla_W tr(Y^TY - Y^TXW - W^TX^TY + W^TX^TXW)$$</p><p>因为$trA = trA^T$，可以化简为：</p><p>$$ \nabla_W F(W) = \nabla_W (Y^TY - 2trY^TXW + trW^TX^TXW)$$</p><p>再利用公式$\nabla_{A^T} trABA^TC = B^TA^TC^T + BA^TC$进行化简，令$A^T = W$，$B = B^T = X^TX$，$C = I$:</p><p>$$ \nabla_W F(W) = \nabla_W (Y^TY - 2trY^TXW + trABA^TC)$$</p><p>展开为：</p><p>$$ \nabla_W F(W) = \nabla_W (Y^TY - 2trY^TXW) + X^TXW + X^TXW$$</p><p>$$ \nabla_W F(W) = \nabla_W (-2trY^TXW) + 2X^TXW$$</p><p>另外根据公式$\nabla_AtrAB = B^T$以及公式$trAB = trBA$可以得到$\nabla_AtrBA = B^T$，所以上式可以继续化简为：</p><p>$$ \nabla_W F(W) = -2X^TY + 2X^TXW$$</p><p>令梯度为0，即可得到$minF(W)$所对应的$W$：</p><p>$$2X^TY = 2X^TXW$$</p><p>$$W = (X^TX)^{-1}X^TY$$</p><p>推导完毕</p><h2 id="岭回归的矩阵运算-1"><a href="#岭回归的矩阵运算-1" class="headerlink" title="岭回归的矩阵运算"></a>岭回归的矩阵运算</h2><p>岭回归的矩阵运算推导跟线性回归类似，只是多了一个范数求导的过程</p><p>目标：</p><p>$$min(||Y-XW||^2_2 + \lambda||W||^2_2)$$</p><h4 id="计算过程-1"><a href="#计算过程-1" class="headerlink" title="计算过程"></a>计算过程</h4><p>$$ F = ||Y-XW||^2_2 + \lambda||W||^2_2 = (Y-XW)^T(Y-XW) + \lambda W^TW$$</p><p>$$ F = (Y^T - W^TX^T)(Y-XW) + \lambda W^TW$$</p><p>$$ F = Y^TY - Y^TXW - W^TX^TY + W^TX^TXW + \lambda W^TW$$</p><p>对F进行求导得到：</p><p>$$ \frac{\partial F}{\partial W} = - Y^TX - Y^TX + (X^TX + X^TX)W + \lambda W$$</p><p>$$ \frac{\partial F}{\partial W} = -2 Y^TX + 2X^TXW + \lambda W$$</p><p>令$\frac{\partial F}{\partial W} = 0$，有：</p><p>$$\frac{\partial F}{\partial W} = -2 Y^TX + 2X^TXW + \lambda W = 0$$</p><p>$$(X^TX + \frac{\lambda I}{2})W = Y^TX$$</p><p>$$W = (X^TX + \frac{\lambda I}{2})^{-1}Y^TX$$</p><p><a href="https://blog.csdn.net/computerme/article/details/50486937" target="_blank" rel="noopener">推导地址</a></p><div class="post-announce">感谢您的阅读，本文由 <a href="http://yoursite.com">阿呆的读书笔记</a> 版权所有。如若转载，请注明出处：阿呆的读书笔记（<a href="http://yoursite.com/2018/04/05/Ridge/">http://yoursite.com/2018/04/05/Ridge/</a>）</div><div class="post__prevs"><div class="post__prev"></div><div class="post__prev post__prev--right"><a href="/2018/04/07/梯度下降与线搜索/" title="线性回归中的梯度下降与一维搜索">线性回归中的梯度下降与一维搜索<i class="iconfont icon-next"></i></a></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">Machine Learning</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2018/04/20/LSTM预测股票走势/" title="LSTM预测股票走势"><div class="item__cover"><img src="undefined" alt="LSTM预测股票走势"></div><div class="item__info"><h3 class="item__title">LSTM预测股票走势</h3><span class="item__text">2018-04-20</span></div></a></li><li class="latest-post-item"><a href="/2018/04/17/极大似然和最小二乘/" title="极大似然和最小二乘"><div class="item__cover"><img src="undefined" alt="极大似然和最小二乘"></div><div class="item__info"><h3 class="item__title">极大似然和最小二乘</h3><span class="item__text">2018-04-17</span></div></a></li><li class="latest-post-item"><a href="/2018/04/17/朴素贝叶斯分类器的Python实现/" title="朴素贝叶斯分类器的Python实现"><div class="item__cover"><img src="undefined" alt="朴素贝叶斯分类器的Python实现"></div><div class="item__info"><h3 class="item__title">朴素贝叶斯分类器的Python实现</h3><span class="item__text">2018-04-17</span></div></a></li><li class="latest-post-item"><a href="/2018/04/15/LSTM预测汇率变化/" title="使用循环神经网络预测汇率涨跌"><div class="item__cover"><img src="undefined" alt="使用循环神经网络预测汇率涨跌"></div><div class="item__info"><h3 class="item__title">使用循环神经网络预测汇率涨跌</h3><span class="item__text">2018-04-15</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/LSTM/">LSTM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-item"><a class="tag-link" href="/tags/回归/">回归</a></li><li class="tag-item"><a class="tag-link" href="/tags/时间序列/">时间序列</a></li><li class="tag-item"><a class="tag-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-item"><a class="tag-link" href="/tags/深度学习/">深度学习</a></li><li class="tag-item"><a class="tag-link" href="/tags/线性回归/">线性回归</a></li><li class="tag-item"><a class="tag-link" href="/tags/贝叶斯/">贝叶斯</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text"></p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span></span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span></span></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>.</p><ul class="footer__social-network clearfix"></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>