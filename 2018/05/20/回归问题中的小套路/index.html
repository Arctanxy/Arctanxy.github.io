<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><title>回归问题中的小套路 |</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="dalalaa"><meta name="designer" content="minfive"><meta name="keywords" content="null"><meta name="description" content="Machine Learning"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="http://yoursite.com/2018/05/20/回归问题中的小套路/index.html"><link rel="icon" type="image/png" href="undefined" sizes="32x32"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="阿呆的读书笔记"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-116821310-1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-116821310-1")</script><link rel="stylesheet" href="/scss/views/page/post.css"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(undefined)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="阿呆的读书笔记" alt="阿呆的读书笔记"><img src="undefined" alt="阿呆的读书笔记"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li><li class="nav__item"><a href="/search" alt="menu.search" title="menu.search">menu.search</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/img/houseprice/house.jpg" alt="回归问题中的小套路"></div><header class="post__info"><h1 class="post__title">回归问题中的小套路</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/Arctanxy">dalalaa</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2018-05-20</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/机器学习/">机器学习</a></li><li class="mark__item"><a href="/tags/回归/">回归</a></li><li class="mark__item"><a href="/tags/特征工程/">特征工程</a></li></ul></div></div></header><div class="post__content"><h1 id="回归问题中的小套路"><a href="#回归问题中的小套路" class="headerlink" title="回归问题中的小套路"></a>回归问题中的小套路</h1><h2 id="Kaggle-Houseprice"><a href="#Kaggle-Houseprice" class="headerlink" title="Kaggle Houseprice"></a>Kaggle Houseprice</h2><p>Kaggle中的入门竞赛Houseprice竞赛是一个经典的回归问题，下面将以其中的特征工程代码演示一下回归问题中的常见套路。</p><h2 id="1-缺失值处理"><a href="#1-缺失值处理" class="headerlink" title="1. 缺失值处理"></a>1. 缺失值处理</h2><p>缺失值处理通常有如下几种方式：</p><ul><li>以特定值填充，有些NAN值具有特殊意义</li><li>使用该特征的均值或中位数进行填充，适用于数值型特征</li><li>使用该特征的众数进行填充，适用于分类型或离散型特征</li><li>参考同类特征进行填充，如Houseprice中可以参考同处一个Neighborhood的特征的数值分布进行缺失值填充</li><li>直接删除，适用于缺失值过多，且该特征方差过小的情况</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 区域因素</span></span><br><span class="line">data[<span class="string">'MSZoning'</span>] = data[<span class="string">'MSZoning'</span>].fillna(data[<span class="string">'MSZoning'</span>].mode().iloc[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 交通地形因素</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">'Street'</span>,<span class="string">'Alley'</span>,<span class="string">'LandContour'</span>,<span class="string">'LandSlope'</span>,<span class="string">'Condition1'</span>,<span class="string">'Condition2'</span>]:</span><br><span class="line">    data[f] = data[f].fillna(data[f].mode().iloc[<span class="number">0</span>])</span><br><span class="line">data[<span class="string">'LotFrontage'</span>] = data[<span class="string">'LotFrontage'</span>].fillna(data[<span class="string">'LotFrontage'</span>].mean())</span><br><span class="line">    <span class="comment"># 房屋总体特征</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">'MasVnrType'</span>,<span class="string">'MasVnrArea'</span>,<span class="string">'Exterior1st'</span>,<span class="string">'Exterior2nd'</span>,<span class="string">'Functional'</span>]:</span><br><span class="line">    data[f] = data[f].fillna(data[f].mode().iloc[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 房屋内部配置</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">'BsmtQual'</span>,<span class="string">'BsmtCond'</span>,<span class="string">'BsmtFinSF1'</span>,<span class="string">'BsmtFinSF2'</span>,<span class="string">'BsmtFullBath'</span>,<span class="string">'BsmtUnfSF'</span>,<span class="string">'BsmtHalfBath'</span>,</span><br><span class="line">        <span class="string">'GarageQual'</span>,<span class="string">'GarageCond'</span>,<span class="string">'PoolQC'</span>,<span class="string">'KitchenQual'</span>,<span class="string">'GarageArea'</span>,<span class="string">'GarageCars'</span>]:</span><br><span class="line">    data[f] = data[f].fillna(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">'BsmtExposure'</span>,<span class="string">'BsmtFinType1'</span>,<span class="string">'BsmtFinType2'</span>,<span class="string">'GarageType'</span>,<span class="string">'GarageYrBlt'</span>,<span class="string">'GarageFinish'</span>,<span class="string">'Fence'</span>,<span class="string">'MiscFeature'</span>]:</span><br><span class="line">    data[f] = data[f].fillna(<span class="string">'None'</span>)</span><br><span class="line">data[<span class="string">'TotalBsmtSF'</span>] = data[<span class="string">'TotalBsmtSF'</span>].fillna(data[<span class="string">'TotalBsmtSF'</span>].mean())</span><br><span class="line">data[<span class="string">'Electrical'</span>] = data[<span class="string">'Electrical'</span>].fillna(data[<span class="string">'Electrical'</span>].mode().iloc[<span class="number">0</span>])</span><br><span class="line">data[<span class="string">'FireplaceQu'</span>] = data[<span class="string">'FireplaceQu'</span>].fillna(<span class="number">0.0</span>)</span><br><span class="line">    <span class="comment"># 销售信息</span></span><br><span class="line">data[<span class="string">'SaleType'</span>] = data[<span class="string">'SaleType'</span>].fillna(data[<span class="string">'SaleType'</span>].mode().iloc[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 其他</span></span><br><span class="line">data[<span class="string">'Utilities'</span>] = data[<span class="string">'Utilities'</span>].fillna(data[<span class="string">'Utilities'</span>].mode().iloc[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><h2 id="2-添加新特征"><a href="#2-添加新特征" class="headerlink" title="2. 添加新特征"></a>2. 添加新特征</h2><p>利用现有的特征，添加新特征，这是机器学习项目中最具创造性的步骤，特征工程决定了最终得分的上限，能否找到项目中的Golden Feature是项目成败的关键。</p><p>这个步骤主要依靠对于特定业务的了解。</p><p>套路的话主要是对特征的组合或者添加多次项转化成多项式回归。</p><blockquote><p>我曾见过一个很生猛的套路：对任意两列特征做加减乘运算，生成新的特征，然后再进行筛选，如果你的电脑性能够强，对项目业务又不太熟悉，不妨尝试一下这种方法，<a href="https://github.com/dataworkshop/xgboost/blob/master/step3.ipynb" target="_blank" rel="noopener">参考代码</a></p></blockquote><p>这个项目中我只添加了三个特征，效果尚可(我曾按国内房产评估方法为每个test样本添加了可比实例价格，效果不好)。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'Remodeled'</span>] = (data[<span class="string">'YearBuilt'</span>] != data[<span class="string">'YearRemodAdd'</span>]) * <span class="number">1</span></span><br><span class="line">data[<span class="string">'Age'</span>] = data[<span class="string">'YrSold'</span>] - data[<span class="string">'YearBuilt'</span>] + <span class="number">1</span></span><br><span class="line">data[<span class="string">'TotalSF'</span>] = data[<span class="string">'TotalBsmtSF'</span>] + data[<span class="string">'1stFlrSF'</span>] + data[<span class="string">'2ndFlrSF'</span>]</span><br></pre></td></tr></table></figure><p></p><h2 id="3-特征处理"><a href="#3-特征处理" class="headerlink" title="3. 特征处理"></a>3. 特征处理</h2><h3 id="连续数值型特征"><a href="#连续数值型特征" class="headerlink" title="连续数值型特征"></a>连续数值型特征</h3><p>对数值型特征的处理方式很简单，主要是对偏态分布的数据进行标准化处理，对于偏度大于某个阈值的特征转为正态分布或者取对数处理(如果觉得设定偏度阈值太麻烦了，可以直接对所有数值型特征进行处理)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">numeric_feats = data.drop([<span class="string">'AVG_PRICE'</span>,<span class="string">'SalePrice'</span>],axis=<span class="number">1</span>).dtypes[(data.dtypes != <span class="string">'object'</span>) &amp; (data.dtypes != <span class="string">'datetime64[ns]'</span>)].index <span class="comment"># 获取数值列</span></span><br><span class="line">skewed_feats = data[numeric_feats].apply(<span class="keyword">lambda</span> x: skew(x))</span><br><span class="line">skewed_feats = skewed_feats[skewed_feats &gt; <span class="number">0.75</span>]</span><br><span class="line">skewed_feats = skewed_feats.index</span><br><span class="line">std = StandardScaler()</span><br><span class="line">data[skewed_feats] = std.fit_transform(data[skewed_feats])</span><br></pre></td></tr></table></figure><blockquote><p>在分类和关联分析问题中，还会有连续变量离散化的操作。</p></blockquote><h3 id="分类型或离散型特征"><a href="#分类型或离散型特征" class="headerlink" title="分类型或离散型特征"></a>分类型或离散型特征</h3><p>字符型的分类特征无法直接带入回归模型中运算，需要进行数值化，然而进行数值化之后，模型会考虑各数值之间的距离：比如把红黄绿三种颜色编号为123，那么模型会认为红色和黄色之间的距离比红色和绿色之间的距离近，从而导致模型偏差。</p><p>通常会采用的方式是对特征进行独热编码，可以通过sklearn中的OneHotEncoder()和pandas中的get_dummies()实现。</p><h2 id="4-特征筛选"><a href="#4-特征筛选" class="headerlink" title="4. 特征筛选"></a>4. 特征筛选</h2><p>特征筛选的筛选主要有两类方式，一种我称之为统计筛选，另一种是模型筛选</p><h3 id="统计筛选"><a href="#统计筛选" class="headerlink" title="统计筛选"></a>统计筛选</h3><ul><li>方差选择法</li><li>相关系数法</li><li>卡方检验法</li><li>互信息法</li></ul><p>这些方法中，方差选择法是单独计算每个特征的方差，选择方差高于阈值的特征。其他三种方法是采用不同的手段计算特征与因变量（预测目标）之间的相关性来筛选特征。</p><h3 id="模型筛选"><a href="#模型筛选" class="headerlink" title="模型筛选"></a>模型筛选</h3><p>模型筛选常见的也有两种方式：</p><ul><li><ol><li>使用模型中的特征重要性进行排序</li></ol></li><li><ol><li>逐步添加或减少特征，如果模型得到改善则保留更改</li></ol></li></ul><p>其实两种方式差不多，只是方法1中的特征重要性只考虑单特征对模型的影响，而方法2中考虑的是不同特征组合的模型效果，在方法2中，本地cv验证方法的选取非常重要。我采用的是第二种方法，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_cv</span><span class="params">(train_data,clf = RidgeCV<span class="params">(alphas=[<span class="number">1e-6</span>,<span class="number">1e-5</span>,<span class="number">1e-4</span>,<span class="number">1e-3</span>,<span class="number">1e-2</span>,<span class="number">1e-1</span>,<span class="number">1</span>])</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    逐步删除特征,运算时间较长，clf尽量选择简单模型</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x = train_data.drop([<span class="string">'SalePrice'</span>,<span class="string">'AVG_PRICE'</span>,<span class="string">'Id'</span>],axis=<span class="number">1</span>)</span><br><span class="line">    y = np.log(train_data[<span class="string">'AVG_PRICE'</span>])</span><br><span class="line">    best_score = check(x,y)</span><br><span class="line">    dropped_col = []</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> tqdm(x.columns):</span><br><span class="line">        score = check(x.drop(col,axis=<span class="number">1</span>),y)</span><br><span class="line">        <span class="keyword">if</span> score &lt;= best_score:</span><br><span class="line">            x = x.drop(col,axis=<span class="number">1</span>)</span><br><span class="line">            best_score = score</span><br><span class="line">            print(score)</span><br><span class="line">            dropped_col.append(col)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    print(x.shape,best_score)</span><br><span class="line">    <span class="keyword">return</span> x.columns</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_cv</span><span class="params">(train_data,clf = RidgeCV<span class="params">(alphas=[<span class="number">1e-6</span>,<span class="number">1e-5</span>,<span class="number">1e-4</span>,<span class="number">1e-3</span>,<span class="number">1e-2</span>,<span class="number">1e-1</span>,<span class="number">1</span>])</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    逐步增加特征</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x = train_data.drop([<span class="string">'SalePrice'</span>,<span class="string">'AVG_PRICE'</span>,<span class="string">'Id'</span>],axis=<span class="number">1</span>)</span><br><span class="line">    y = np.log(train_data[<span class="string">'AVG_PRICE'</span>])</span><br><span class="line">    best_score = np.inf</span><br><span class="line">    <span class="comment"># 先寻找最好的单特征</span></span><br><span class="line">    best_col = <span class="string">""</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> tqdm(x.columns):</span><br><span class="line">        score = check(x[col].reshape(<span class="number">-1</span>,<span class="number">1</span>),y)</span><br><span class="line">        <span class="keyword">if</span> score &lt; best_score:</span><br><span class="line">            best_score = score</span><br><span class="line">            best_col = col</span><br><span class="line">            print(score)</span><br><span class="line">    x_new = pd.DataFrame(&#123;</span><br><span class="line">        best_col:x[best_col]</span><br><span class="line">    &#125;)</span><br><span class="line">    print(<span class="string">'===best_col=='</span>,best_col)</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> tqdm(x.drop(best_col,axis=<span class="number">1</span>).columns):</span><br><span class="line">        x_new[col] = x[col] <span class="comment"># 这一列莫名其妙地加到了行上面</span></span><br><span class="line">        score = check(x_new,y)</span><br><span class="line">        <span class="keyword">if</span> score &lt; best_score:</span><br><span class="line">            best_score = score</span><br><span class="line">            print(col,score)</span><br><span class="line">        <span class="keyword">elif</span> len(x_new.shape) &gt; <span class="number">1</span>:</span><br><span class="line">            x_new = x_new.drop(col,axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x_new.columns</span><br></pre></td></tr></table></figure><h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><ul><li>PCA</li><li>LDA</li></ul><p>降维通常是用来减少特征中的线性相关量，控制模型中的维度，通常使用与模型中特征量过大，又不好删除的情况（不确定哪些因素对模型没有用）。这个方法我暂时没有用到。</p><h2 id="5-模型调参"><a href="#5-模型调参" class="headerlink" title="5. 模型调参"></a>5. 模型调参</h2><p>很多模型中都有超参数，就是那种不确定会对模型影响不明确的因素。sklearn提供了两种调参方式，分别是网格搜索GridSearchCV()和随机搜索RandomizedSearchCV()。GridSearchCV效果更稳定，RandomizedSearchCV就有点看人品了，效果好的时候比GridSearchCV好，差的时候会很差。</p><p>下面是我用的调参参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">rid = search_model(Ridge(),x,y,params = &#123;</span><br><span class="line">    <span class="string">'alpha'</span>: [<span class="number">1e-6</span>,<span class="number">1e-5</span>,<span class="number">1e-4</span>,<span class="number">1e-3</span>,<span class="number">1e-2</span>,<span class="number">1e-1</span>,<span class="number">1</span>],</span><br><span class="line">    <span class="string">'fit_intercept'</span>: [<span class="keyword">True</span>,<span class="keyword">False</span>],</span><br><span class="line">    <span class="string">'normalize'</span>: [<span class="keyword">True</span>,<span class="keyword">False</span>],</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">las = search_model(Lasso(),x,y,params = &#123;</span><br><span class="line">    <span class="string">'alpha'</span>:[<span class="number">1e-6</span>,<span class="number">1e-5</span>,<span class="number">1e-4</span>,<span class="number">1e-3</span>,<span class="number">1e-2</span>,<span class="number">1e-1</span>,<span class="number">1</span>],</span><br><span class="line">    <span class="string">'fit_intercept'</span>: [<span class="keyword">True</span>,<span class="keyword">False</span>],</span><br><span class="line">    <span class="string">'normalize'</span>: [<span class="keyword">True</span>,<span class="keyword">False</span>],</span><br><span class="line">    <span class="string">'max_iter'</span>:[<span class="number">100</span>,<span class="number">300</span>,<span class="number">500</span>]</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">xg = search_model(XGBRegressor(),x,y,params = &#123;</span><br><span class="line">    <span class="string">'learning_rate'</span>:[<span class="number">0.1</span>],</span><br><span class="line">    <span class="string">'max_depth'</span>:[<span class="number">2</span>],</span><br><span class="line">    <span class="string">'n_estimators'</span>:[<span class="number">500</span>],</span><br><span class="line">    <span class="string">'reg_alpha'</span>:[<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>],</span><br><span class="line">    <span class="string">'reg_lambda'</span>:[<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>]</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">rf = search_model(RandomForestRegressor(),x,y,params=&#123;</span><br><span class="line">    <span class="string">'n_estimators'</span>:[<span class="number">300</span>,<span class="number">500</span>,<span class="number">800</span>],</span><br><span class="line">    <span class="string">'max_features'</span>:[<span class="number">0.5</span>,<span class="string">'sqrt'</span>,<span class="number">0.8</span>],</span><br><span class="line">    <span class="string">'min_samples_leaf'</span>:[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">    <span class="string">'n_jobs'</span>:[<span class="number">-1</span>],</span><br><span class="line">    <span class="string">'max_depth'</span>:[<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">11</span>]</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">krr = search_model(KernelRidge(),x,y,params=&#123;</span><br><span class="line">    <span class="string">'alpha'</span>:[<span class="number">1e-4</span>,<span class="number">1e-3</span>,<span class="number">1e-2</span>,<span class="number">1e-1</span>,<span class="number">1e0</span>,<span class="number">1e1</span>],</span><br><span class="line">    <span class="string">'kernel'</span>:[<span class="string">'linear'</span>,<span class="string">'polynomial'</span>,<span class="string">'rbf'</span>],</span><br><span class="line">    <span class="string">'degree'</span>:[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">gbd = search_model(GradientBoostingRegressor(),x,y,params = &#123;</span><br><span class="line">    <span class="string">'loss'</span>:[<span class="string">'ls'</span>, <span class="string">'lad'</span>, <span class="string">'huber'</span>, <span class="string">'quantile'</span>],</span><br><span class="line">    <span class="string">'learning_rate'</span>:[<span class="number">1e-4</span>,<span class="number">1e-3</span>,<span class="number">1e-2</span>,<span class="number">1e-1</span>],</span><br><span class="line">    <span class="string">'n_estimators'</span>:[<span class="number">100</span>,<span class="number">200</span>,<span class="number">400</span>],</span><br><span class="line">    <span class="string">'criterion'</span>:[<span class="string">'mse'</span>],</span><br><span class="line">    <span class="string">'max_features'</span>:[<span class="string">'sqrt'</span>,<span class="string">'log2'</span>]</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h2 id="6-模型融合"><a href="#6-模型融合" class="headerlink" title="6. 模型融合"></a>6. 模型融合</h2><p>模型融合的目的是提高模型的泛化能力，通常会采用得分相近、但是原理相差较大的几个模型进行融合，比如回归模型中可以用Rdige/Lasso回归 + 随机森林 + xgboost 这样的组合方式。</p><p>组合方式也有多种：</p><h3 id="Average"><a href="#Average" class="headerlink" title="Average"></a>Average</h3><p>最简单的融合方式，就是把多个线性模型的结果进行线性组合。如果在分类问题中可以使用类似的Voting方法，这种简单又有效的方法当然要尝试一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">voting_predict</span><span class="params">(models,test,weights=<span class="string">'auto'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''表决结果'''</span></span><br><span class="line">    <span class="keyword">if</span> weights == <span class="string">'auto'</span>:</span><br><span class="line">        weights = [<span class="number">1</span>/len(models) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(models))]</span><br><span class="line">    weights = np.array(weights).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">    predictions = np.zeros((test.shape[<span class="number">0</span>],len(models)))</span><br><span class="line">    <span class="keyword">for</span> i,m <span class="keyword">in</span> enumerate(models):</span><br><span class="line">        yp = m.predict(test.drop(<span class="string">'Id'</span>,axis=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># predictions.append(yp)</span></span><br><span class="line">        predictions[:,i] = yp</span><br><span class="line">    <span class="keyword">return</span> np.squeeze(np.dot(predictions,weights))</span><br></pre></td></tr></table></figure><h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>多次从总样本中有放回地抽取样本，通过得到的子样本建立多个子模型，然后使用Average将这些子模型进行融合。随机森林算法就是衍生于bagging算法</p><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>多次迭代训练，每次训练完之后，将预测效果较差的样本的权重加大，然后再对训练出来的子模型结果进行加权的线性组合（与Average类似），sklearn中提供了Adaboost和GBDT函数，可以直接调用。</p><h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><p>Stacking是比较难描述的算法，原理如下图所示：<br><img src="https://pic4.zhimg.com/v2-84dbc338e11fb89320f2ba310ad69ceb_b.jpg" alt="Stacking原理图"><br>在Python中没有现成的模块可用，需要自己写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">stack_model</span>:</span></span><br><span class="line">    <span class="string">'''使用KFold的方式将数据集划分为5个部分，使用每个basemodel训练五次，</span></span><br><span class="line"><span class="string">    再预测五次，合并得到一个predict_price，作为mergemodel中的自变量'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,base_models,merge_model,n_folds = <span class="number">5</span>)</span>:</span></span><br><span class="line">        self.base_models = base_models</span><br><span class="line">        self.merge_model = merge_model</span><br><span class="line">        self.n_folds = n_folds</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        self.fitted_models = [list() <span class="keyword">for</span> x <span class="keyword">in</span> self.base_models] <span class="comment"># 用于存储训练之后的模型</span></span><br><span class="line">        kfold = KFold(n_splits = self.n_folds,shuffle = <span class="keyword">True</span>)</span><br><span class="line">        out_of_fold_predictions = np.zeros((x.shape[<span class="number">0</span>],len(self.base_models)))</span><br><span class="line">        <span class="keyword">for</span> i,model <span class="keyword">in</span> enumerate(self.base_models):</span><br><span class="line">            <span class="keyword">for</span> train_index,valid_index <span class="keyword">in</span> kfold.split(x,y):</span><br><span class="line">                instance = clone(model)</span><br><span class="line">                instance.fit(x.iloc[train_index],y.iloc[train_index])</span><br><span class="line">                self.fitted_models[i].append(instance)</span><br><span class="line">                y_pred = instance.predict(x.iloc[valid_index])</span><br><span class="line">                out_of_fold_predictions[valid_index,i] = y_pred</span><br><span class="line">        self.merge_model.fit(out_of_fold_predictions,y)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        merge_features = np.column_stack([</span><br><span class="line">            np.column_stack([</span><br><span class="line">                model.predict(x) <span class="keyword">for</span> model <span class="keyword">in</span> models</span><br><span class="line">            ]).mean(axis=<span class="number">1</span>) <span class="keyword">for</span> models <span class="keyword">in</span> self.fitted_models</span><br><span class="line">        ])</span><br><span class="line">        <span class="keyword">return</span> self.merge_model.predict(merge_features)</span><br></pre></td></tr></table></figure><p>整个思路就是这样，现在模型还在调整中，如果效果比较好的话，我会把源代码分享出来。</p><div class="post-announce">感谢您的阅读，本文由 <a href="http://yoursite.com">阿呆的读书笔记</a> 版权所有。如若转载，请注明出处：阿呆的读书笔记（<a href="http://yoursite.com/2018/05/20/回归问题中的小套路/">http://yoursite.com/2018/05/20/回归问题中的小套路/</a>）</div><div class="post__prevs"><div class="post__prev"><a href="/2018/04/30/OpenCV简单用法（一）/" title="OpenCV简单用法（一）"><i class="iconfont icon-prev"></i>OpenCV简单用法（一）</a></div><div class="post__prev post__prev--right"></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">Machine Learning</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2018/05/20/回归问题中的小套路/" title="回归问题中的小套路"><div class="item__cover"><img src="/img/houseprice/house.jpg" alt="回归问题中的小套路"></div><div class="item__info"><h3 class="item__title">回归问题中的小套路</h3><span class="item__text">2018-05-20</span></div></a></li><li class="latest-post-item"><a href="/2018/04/30/OpenCV简单用法（一）/" title="OpenCV简单用法（一）"><div class="item__cover"><img src="/img/OpenCV简单用法/Diablo.jpg" alt="OpenCV简单用法（一）"></div><div class="item__info"><h3 class="item__title">OpenCV简单用法（一）</h3><span class="item__text">2018-04-30</span></div></a></li><li class="latest-post-item"><a href="/2018/04/30/OpenCV简单用法（二）/" title="OpenCV简单用法（二）"><div class="item__cover"><img src="/img/OpenCV简单用法/Atreus.jpg" alt="OpenCV简单用法（二）"></div><div class="item__info"><h3 class="item__title">OpenCV简单用法（二）</h3><span class="item__text">2018-04-30</span></div></a></li><li class="latest-post-item"><a href="/2018/04/20/LSTM预测股票走势/" title="LSTM预测股票走势"><div class="item__cover"><img src="https://github.com/Arctanxy/learning_notes/blob/master/study/ttfunds/prediction.jpg?raw=true" alt="LSTM预测股票走势"></div><div class="item__info"><h3 class="item__title">LSTM预测股票走势</h3><span class="item__text">2018-04-20</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/LSTM/">LSTM</a></li><li class="tag-item"><a class="tag-link" href="/tags/OpenCV/">OpenCV</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-item"><a class="tag-link" href="/tags/回归/">回归</a></li><li class="tag-item"><a class="tag-link" href="/tags/图像处理/">图像处理</a></li><li class="tag-item"><a class="tag-link" href="/tags/时间序列/">时间序列</a></li><li class="tag-item"><a class="tag-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-item"><a class="tag-link" href="/tags/深度学习/">深度学习</a></li><li class="tag-item"><a class="tag-link" href="/tags/特征工程/">特征工程</a></li><li class="tag-item"><a class="tag-link" href="/tags/线性回归/">线性回归</a></li><li class="tag-item"><a class="tag-link" href="/tags/贝叶斯/">贝叶斯</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text"></p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span></span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span></span></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>.</p><ul class="footer__social-network clearfix"></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></body></html>