<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>阿呆的读书笔记</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-05-20T14:31:22.686Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>dalalaa</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>回归问题中的小套路</title>
    <link href="http://yoursite.com/2018/05/20/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E4%B8%AD%E7%9A%84%E5%B0%8F%E5%A5%97%E8%B7%AF/"/>
    <id>http://yoursite.com/2018/05/20/回归问题中的小套路/</id>
    <published>2018-05-20T02:20:18.000Z</published>
    <updated>2018-05-20T14:31:22.686Z</updated>
    
    <content type="html"><![CDATA[<h1 id="回归问题中的小套路"><a href="#回归问题中的小套路" class="headerlink" title="回归问题中的小套路"></a>回归问题中的小套路</h1><h2 id="Kaggle-Houseprice"><a href="#Kaggle-Houseprice" class="headerlink" title="Kaggle Houseprice"></a>Kaggle Houseprice</h2><p>Kaggle中的入门竞赛Houseprice竞赛是一个经典的回归问题，下面将以其中的特征工程代码演示一下回归问题中的常见套路。</p><h2 id="1-缺失值处理"><a href="#1-缺失值处理" class="headerlink" title="1. 缺失值处理"></a>1. 缺失值处理</h2><p>缺失值处理通常有如下几种方式：</p><ul><li>以特定值填充，有些NAN值具有特殊意义</li><li>使用该特征的均值或中位数进行填充，适用于数值型特征</li><li>使用该特征的众数进行填充，适用于分类型或离散型特征</li><li>参考同类特征进行填充，如Houseprice中可以参考同处一个Neighborhood的特征的数值分布进行缺失值填充</li><li>直接删除，适用于缺失值过多，且该特征方差过小的情况</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 区域因素</span></span><br><span class="line">data[<span class="string">'MSZoning'</span>] = data[<span class="string">'MSZoning'</span>].fillna(data[<span class="string">'MSZoning'</span>].mode().iloc[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 交通地形因素</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">'Street'</span>,<span class="string">'Alley'</span>,<span class="string">'LandContour'</span>,<span class="string">'LandSlope'</span>,<span class="string">'Condition1'</span>,<span class="string">'Condition2'</span>]:</span><br><span class="line">    data[f] = data[f].fillna(data[f].mode().iloc[<span class="number">0</span>])</span><br><span class="line">data[<span class="string">'LotFrontage'</span>] = data[<span class="string">'LotFrontage'</span>].fillna(data[<span class="string">'LotFrontage'</span>].mean())</span><br><span class="line">    <span class="comment"># 房屋总体特征</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">'MasVnrType'</span>,<span class="string">'MasVnrArea'</span>,<span class="string">'Exterior1st'</span>,<span class="string">'Exterior2nd'</span>,<span class="string">'Functional'</span>]:</span><br><span class="line">    data[f] = data[f].fillna(data[f].mode().iloc[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 房屋内部配置</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">'BsmtQual'</span>,<span class="string">'BsmtCond'</span>,<span class="string">'BsmtFinSF1'</span>,<span class="string">'BsmtFinSF2'</span>,<span class="string">'BsmtFullBath'</span>,<span class="string">'BsmtUnfSF'</span>,<span class="string">'BsmtHalfBath'</span>,</span><br><span class="line">        <span class="string">'GarageQual'</span>,<span class="string">'GarageCond'</span>,<span class="string">'PoolQC'</span>,<span class="string">'KitchenQual'</span>,<span class="string">'GarageArea'</span>,<span class="string">'GarageCars'</span>]:</span><br><span class="line">    data[f] = data[f].fillna(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">'BsmtExposure'</span>,<span class="string">'BsmtFinType1'</span>,<span class="string">'BsmtFinType2'</span>,<span class="string">'GarageType'</span>,<span class="string">'GarageYrBlt'</span>,<span class="string">'GarageFinish'</span>,<span class="string">'Fence'</span>,<span class="string">'MiscFeature'</span>]:</span><br><span class="line">    data[f] = data[f].fillna(<span class="string">'None'</span>)</span><br><span class="line">data[<span class="string">'TotalBsmtSF'</span>] = data[<span class="string">'TotalBsmtSF'</span>].fillna(data[<span class="string">'TotalBsmtSF'</span>].mean())</span><br><span class="line">data[<span class="string">'Electrical'</span>] = data[<span class="string">'Electrical'</span>].fillna(data[<span class="string">'Electrical'</span>].mode().iloc[<span class="number">0</span>])</span><br><span class="line">data[<span class="string">'FireplaceQu'</span>] = data[<span class="string">'FireplaceQu'</span>].fillna(<span class="number">0.0</span>)</span><br><span class="line">    <span class="comment"># 销售信息</span></span><br><span class="line">data[<span class="string">'SaleType'</span>] = data[<span class="string">'SaleType'</span>].fillna(data[<span class="string">'SaleType'</span>].mode().iloc[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 其他</span></span><br><span class="line">data[<span class="string">'Utilities'</span>] = data[<span class="string">'Utilities'</span>].fillna(data[<span class="string">'Utilities'</span>].mode().iloc[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><h2 id="2-添加新特征"><a href="#2-添加新特征" class="headerlink" title="2. 添加新特征"></a>2. 添加新特征</h2><p>利用现有的特征，添加新特征，这是机器学习项目中最具创造性的步骤，特征工程决定了最终得分的上限，能否找到项目中的Golden Feature是项目成败的关键。</p><p>这个步骤主要依靠对于特定业务的了解。</p><p>套路的话主要是对特征的组合或者添加多次项转化成多项式回归。</p><blockquote><p>我曾见过一个很生猛的套路：对任意两列特征做加减乘运算，生成新的特征，然后再进行筛选，如果你的电脑性能够强，对项目业务又不太熟悉，不妨尝试一下这种方法，<a href="https://github.com/dataworkshop/xgboost/blob/master/step3.ipynb" target="_blank" rel="noopener">参考代码</a></p></blockquote><p>这个项目中我只添加了三个特征，效果尚可(我曾按国内房产评估方法为每个test样本添加了可比实例价格，效果不好)。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'Remodeled'</span>] = (data[<span class="string">'YearBuilt'</span>] != data[<span class="string">'YearRemodAdd'</span>]) * <span class="number">1</span></span><br><span class="line">data[<span class="string">'Age'</span>] = data[<span class="string">'YrSold'</span>] - data[<span class="string">'YearBuilt'</span>] + <span class="number">1</span></span><br><span class="line">data[<span class="string">'TotalSF'</span>] = data[<span class="string">'TotalBsmtSF'</span>] + data[<span class="string">'1stFlrSF'</span>] + data[<span class="string">'2ndFlrSF'</span>]</span><br></pre></td></tr></table></figure></p><h2 id="3-特征处理"><a href="#3-特征处理" class="headerlink" title="3. 特征处理"></a>3. 特征处理</h2><h3 id="连续数值型特征"><a href="#连续数值型特征" class="headerlink" title="连续数值型特征"></a>连续数值型特征</h3><p>对数值型特征的处理方式很简单，主要是对偏态分布的数据进行标准化处理，对于偏度大于某个阈值的特征转为正态分布或者取对数处理(如果觉得设定偏度阈值太麻烦了，可以直接对所有数值型特征进行处理)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">numeric_feats = data.drop([<span class="string">'AVG_PRICE'</span>,<span class="string">'SalePrice'</span>],axis=<span class="number">1</span>).dtypes[(data.dtypes != <span class="string">'object'</span>) &amp; (data.dtypes != <span class="string">'datetime64[ns]'</span>)].index <span class="comment"># 获取数值列</span></span><br><span class="line">skewed_feats = data[numeric_feats].apply(<span class="keyword">lambda</span> x: skew(x))</span><br><span class="line">skewed_feats = skewed_feats[skewed_feats &gt; <span class="number">0.75</span>]</span><br><span class="line">skewed_feats = skewed_feats.index</span><br><span class="line">std = StandardScaler()</span><br><span class="line">data[skewed_feats] = std.fit_transform(data[skewed_feats])</span><br></pre></td></tr></table></figure><blockquote><p>在分类和关联分析问题中，还会有连续变量离散化的操作。</p></blockquote><h3 id="分类型或离散型特征"><a href="#分类型或离散型特征" class="headerlink" title="分类型或离散型特征"></a>分类型或离散型特征</h3><p>字符型的分类特征无法直接带入回归模型中运算，需要进行数值化，然而进行数值化之后，模型会考虑各数值之间的距离：比如把红黄绿三种颜色编号为123，那么模型会认为红色和黄色之间的距离比红色和绿色之间的距离近，从而导致模型偏差。</p><p>通常会采用的方式是对特征进行独热编码，可以通过sklearn中的OneHotEncoder()和pandas中的get_dummies()实现。</p><h2 id="4-特征筛选"><a href="#4-特征筛选" class="headerlink" title="4. 特征筛选"></a>4. 特征筛选</h2><p>特征筛选的筛选主要有两类方式，一种我称之为统计筛选，另一种是模型筛选</p><h3 id="统计筛选"><a href="#统计筛选" class="headerlink" title="统计筛选"></a>统计筛选</h3><ul><li>方差选择法</li><li>相关系数法</li><li>卡方检验法</li><li>互信息法</li></ul><p>这些方法中，方差选择法是单独计算每个特征的方差，选择方差高于阈值的特征。其他三种方法是采用不同的手段计算特征与因变量（预测目标）之间的相关性来筛选特征。</p><h3 id="模型筛选"><a href="#模型筛选" class="headerlink" title="模型筛选"></a>模型筛选</h3><p>模型筛选常见的也有两种方式：</p><ul><li><ol><li>使用模型中的特征重要性进行排序</li></ol></li><li><ol><li>逐步添加或减少特征，如果模型得到改善则保留更改</li></ol></li></ul><p>其实两种方式差不多，只是方法1中的特征重要性只考虑单特征对模型的影响，而方法2中考虑的是不同特征组合的模型效果，在方法2中，本地cv验证方法的选取非常重要。我采用的是第二种方法，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_cv</span><span class="params">(train_data,clf = RidgeCV<span class="params">(alphas=[<span class="number">1e-6</span>,<span class="number">1e-5</span>,<span class="number">1e-4</span>,<span class="number">1e-3</span>,<span class="number">1e-2</span>,<span class="number">1e-1</span>,<span class="number">1</span>])</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    逐步删除特征,运算时间较长，clf尽量选择简单模型</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x = train_data.drop([<span class="string">'SalePrice'</span>,<span class="string">'AVG_PRICE'</span>,<span class="string">'Id'</span>],axis=<span class="number">1</span>)</span><br><span class="line">    y = np.log(train_data[<span class="string">'AVG_PRICE'</span>])</span><br><span class="line">    best_score = check(x,y)</span><br><span class="line">    dropped_col = []</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> tqdm(x.columns):</span><br><span class="line">        score = check(x.drop(col,axis=<span class="number">1</span>),y)</span><br><span class="line">        <span class="keyword">if</span> score &lt;= best_score:</span><br><span class="line">            x = x.drop(col,axis=<span class="number">1</span>)</span><br><span class="line">            best_score = score</span><br><span class="line">            print(score)</span><br><span class="line">            dropped_col.append(col)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    print(x.shape,best_score)</span><br><span class="line">    <span class="keyword">return</span> x.columns</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_cv</span><span class="params">(train_data,clf = RidgeCV<span class="params">(alphas=[<span class="number">1e-6</span>,<span class="number">1e-5</span>,<span class="number">1e-4</span>,<span class="number">1e-3</span>,<span class="number">1e-2</span>,<span class="number">1e-1</span>,<span class="number">1</span>])</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    逐步增加特征</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x = train_data.drop([<span class="string">'SalePrice'</span>,<span class="string">'AVG_PRICE'</span>,<span class="string">'Id'</span>],axis=<span class="number">1</span>)</span><br><span class="line">    y = np.log(train_data[<span class="string">'AVG_PRICE'</span>])</span><br><span class="line">    best_score = np.inf</span><br><span class="line">    <span class="comment"># 先寻找最好的单特征</span></span><br><span class="line">    best_col = <span class="string">""</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> tqdm(x.columns):</span><br><span class="line">        score = check(x[col].reshape(<span class="number">-1</span>,<span class="number">1</span>),y)</span><br><span class="line">        <span class="keyword">if</span> score &lt; best_score:</span><br><span class="line">            best_score = score</span><br><span class="line">            best_col = col</span><br><span class="line">            print(score)</span><br><span class="line">    x_new = pd.DataFrame(&#123;</span><br><span class="line">        best_col:x[best_col]</span><br><span class="line">    &#125;)</span><br><span class="line">    print(<span class="string">'===best_col=='</span>,best_col)</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> tqdm(x.drop(best_col,axis=<span class="number">1</span>).columns):</span><br><span class="line">        x_new[col] = x[col] <span class="comment"># 这一列莫名其妙地加到了行上面</span></span><br><span class="line">        score = check(x_new,y)</span><br><span class="line">        <span class="keyword">if</span> score &lt; best_score:</span><br><span class="line">            best_score = score</span><br><span class="line">            print(col,score)</span><br><span class="line">        <span class="keyword">elif</span> len(x_new.shape) &gt; <span class="number">1</span>:</span><br><span class="line">            x_new = x_new.drop(col,axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x_new.columns</span><br></pre></td></tr></table></figure><h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><ul><li>PCA</li><li>LDA</li></ul><p>降维通常是用来减少特征中的线性相关量，控制模型中的维度，通常使用与模型中特征量过大，又不好删除的情况（不确定哪些因素对模型没有用）。这个方法我暂时没有用到。</p><h2 id="5-模型调参"><a href="#5-模型调参" class="headerlink" title="5. 模型调参"></a>5. 模型调参</h2><p>很多模型中都有超参数，就是那种不确定会对模型影响不明确的因素。sklearn提供了两种调参方式，分别是网格搜索GridSearchCV()和随机搜索RandomizedSearchCV()。GridSearchCV效果更稳定，RandomizedSearchCV就有点看人品了，效果好的时候比GridSearchCV好，差的时候会很差。</p><p>下面是我用的调参参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">rid = search_model(Ridge(),x,y,params = &#123;</span><br><span class="line">    <span class="string">'alpha'</span>: [<span class="number">1e-6</span>,<span class="number">1e-5</span>,<span class="number">1e-4</span>,<span class="number">1e-3</span>,<span class="number">1e-2</span>,<span class="number">1e-1</span>,<span class="number">1</span>],</span><br><span class="line">    <span class="string">'fit_intercept'</span>: [<span class="keyword">True</span>,<span class="keyword">False</span>],</span><br><span class="line">    <span class="string">'normalize'</span>: [<span class="keyword">True</span>,<span class="keyword">False</span>],</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">las = search_model(Lasso(),x,y,params = &#123;</span><br><span class="line">    <span class="string">'alpha'</span>:[<span class="number">1e-6</span>,<span class="number">1e-5</span>,<span class="number">1e-4</span>,<span class="number">1e-3</span>,<span class="number">1e-2</span>,<span class="number">1e-1</span>,<span class="number">1</span>],</span><br><span class="line">    <span class="string">'fit_intercept'</span>: [<span class="keyword">True</span>,<span class="keyword">False</span>],</span><br><span class="line">    <span class="string">'normalize'</span>: [<span class="keyword">True</span>,<span class="keyword">False</span>],</span><br><span class="line">    <span class="string">'max_iter'</span>:[<span class="number">100</span>,<span class="number">300</span>,<span class="number">500</span>]</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">xg = search_model(XGBRegressor(),x,y,params = &#123;</span><br><span class="line">    <span class="string">'learning_rate'</span>:[<span class="number">0.1</span>],</span><br><span class="line">    <span class="string">'max_depth'</span>:[<span class="number">2</span>],</span><br><span class="line">    <span class="string">'n_estimators'</span>:[<span class="number">500</span>],</span><br><span class="line">    <span class="string">'reg_alpha'</span>:[<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>],</span><br><span class="line">    <span class="string">'reg_lambda'</span>:[<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>]</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">rf = search_model(RandomForestRegressor(),x,y,params=&#123;</span><br><span class="line">    <span class="string">'n_estimators'</span>:[<span class="number">300</span>,<span class="number">500</span>,<span class="number">800</span>],</span><br><span class="line">    <span class="string">'max_features'</span>:[<span class="number">0.5</span>,<span class="string">'sqrt'</span>,<span class="number">0.8</span>],</span><br><span class="line">    <span class="string">'min_samples_leaf'</span>:[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">    <span class="string">'n_jobs'</span>:[<span class="number">-1</span>],</span><br><span class="line">    <span class="string">'max_depth'</span>:[<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">11</span>]</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">krr = search_model(KernelRidge(),x,y,params=&#123;</span><br><span class="line">    <span class="string">'alpha'</span>:[<span class="number">1e-4</span>,<span class="number">1e-3</span>,<span class="number">1e-2</span>,<span class="number">1e-1</span>,<span class="number">1e0</span>,<span class="number">1e1</span>],</span><br><span class="line">    <span class="string">'kernel'</span>:[<span class="string">'linear'</span>,<span class="string">'polynomial'</span>,<span class="string">'rbf'</span>],</span><br><span class="line">    <span class="string">'degree'</span>:[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">gbd = search_model(GradientBoostingRegressor(),x,y,params = &#123;</span><br><span class="line">    <span class="string">'loss'</span>:[<span class="string">'ls'</span>, <span class="string">'lad'</span>, <span class="string">'huber'</span>, <span class="string">'quantile'</span>],</span><br><span class="line">    <span class="string">'learning_rate'</span>:[<span class="number">1e-4</span>,<span class="number">1e-3</span>,<span class="number">1e-2</span>,<span class="number">1e-1</span>],</span><br><span class="line">    <span class="string">'n_estimators'</span>:[<span class="number">100</span>,<span class="number">200</span>,<span class="number">400</span>],</span><br><span class="line">    <span class="string">'criterion'</span>:[<span class="string">'mse'</span>],</span><br><span class="line">    <span class="string">'max_features'</span>:[<span class="string">'sqrt'</span>,<span class="string">'log2'</span>]</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h2 id="6-模型融合"><a href="#6-模型融合" class="headerlink" title="6. 模型融合"></a>6. 模型融合</h2><p>模型融合的目的是提高模型的泛化能力，通常会采用得分相近、但是原理相差较大的几个模型进行融合，比如回归模型中可以用Rdige/Lasso回归 + 随机森林 + xgboost 这样的组合方式。</p><p>组合方式也有多种：</p><h3 id="Average"><a href="#Average" class="headerlink" title="Average"></a>Average</h3><p>最简单的融合方式，就是把多个线性模型的结果进行线性组合。如果在分类问题中可以使用类似的Voting方法，这种简单又有效的方法当然要尝试一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">voting_predict</span><span class="params">(models,test,weights=<span class="string">'auto'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''表决结果'''</span></span><br><span class="line">    <span class="keyword">if</span> weights == <span class="string">'auto'</span>:</span><br><span class="line">        weights = [<span class="number">1</span>/len(models) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(models))]</span><br><span class="line">    weights = np.array(weights).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">    predictions = np.zeros((test.shape[<span class="number">0</span>],len(models)))</span><br><span class="line">    <span class="keyword">for</span> i,m <span class="keyword">in</span> enumerate(models):</span><br><span class="line">        yp = m.predict(test.drop(<span class="string">'Id'</span>,axis=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># predictions.append(yp)</span></span><br><span class="line">        predictions[:,i] = yp</span><br><span class="line">    <span class="keyword">return</span> np.squeeze(np.dot(predictions,weights))</span><br></pre></td></tr></table></figure><h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>多次从总样本中有放回地抽取样本，通过得到的子样本建立多个子模型，然后使用Average将这些子模型进行融合。随机森林算法就是衍生于bagging算法</p><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>多次迭代训练，每次训练完之后，将预测效果较差的样本的权重加大，然后再对训练出来的子模型结果进行加权的线性组合（与Average类似），sklearn中提供了Adaboost和GBDT函数，可以直接调用。</p><h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><p>Stacking是比较难描述的算法，原理如下图所示：<br><img src="https://pic4.zhimg.com/v2-84dbc338e11fb89320f2ba310ad69ceb_b.jpg" alt="Stacking原理图"><br>在Python中没有现成的模块可用，需要自己写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">stack_model</span>:</span></span><br><span class="line">    <span class="string">'''使用KFold的方式将数据集划分为5个部分，使用每个basemodel训练五次，</span></span><br><span class="line"><span class="string">    再预测五次，合并得到一个predict_price，作为mergemodel中的自变量'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,base_models,merge_model,n_folds = <span class="number">5</span>)</span>:</span></span><br><span class="line">        self.base_models = base_models</span><br><span class="line">        self.merge_model = merge_model</span><br><span class="line">        self.n_folds = n_folds</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        self.fitted_models = [list() <span class="keyword">for</span> x <span class="keyword">in</span> self.base_models] <span class="comment"># 用于存储训练之后的模型</span></span><br><span class="line">        kfold = KFold(n_splits = self.n_folds,shuffle = <span class="keyword">True</span>)</span><br><span class="line">        out_of_fold_predictions = np.zeros((x.shape[<span class="number">0</span>],len(self.base_models)))</span><br><span class="line">        <span class="keyword">for</span> i,model <span class="keyword">in</span> enumerate(self.base_models):</span><br><span class="line">            <span class="keyword">for</span> train_index,valid_index <span class="keyword">in</span> kfold.split(x,y):</span><br><span class="line">                instance = clone(model)</span><br><span class="line">                instance.fit(x.iloc[train_index],y.iloc[train_index])</span><br><span class="line">                self.fitted_models[i].append(instance)</span><br><span class="line">                y_pred = instance.predict(x.iloc[valid_index])</span><br><span class="line">                out_of_fold_predictions[valid_index,i] = y_pred</span><br><span class="line">        self.merge_model.fit(out_of_fold_predictions,y)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        merge_features = np.column_stack([</span><br><span class="line">            np.column_stack([</span><br><span class="line">                model.predict(x) <span class="keyword">for</span> model <span class="keyword">in</span> models</span><br><span class="line">            ]).mean(axis=<span class="number">1</span>) <span class="keyword">for</span> models <span class="keyword">in</span> self.fitted_models</span><br><span class="line">        ])</span><br><span class="line">        <span class="keyword">return</span> self.merge_model.predict(merge_features)</span><br></pre></td></tr></table></figure><p>整个思路就是这样，现在模型还在调整中，如果效果比较好的话，我会把源代码分享出来。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;回归问题中的小套路&quot;&gt;&lt;a href=&quot;#回归问题中的小套路&quot; class=&quot;headerlink&quot; title=&quot;回归问题中的小套路&quot;&gt;&lt;/a&gt;回归问题中的小套路&lt;/h1&gt;&lt;h2 id=&quot;Kaggle-Houseprice&quot;&gt;&lt;a href=&quot;#Kaggle-H
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="回归" scheme="http://yoursite.com/tags/%E5%9B%9E%E5%BD%92/"/>
    
      <category term="特征工程" scheme="http://yoursite.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>OpenCV简单用法（一）</title>
    <link href="http://yoursite.com/2018/04/30/OpenCV%E7%AE%80%E5%8D%95%E7%94%A8%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2018/04/30/OpenCV简单用法（一）/</id>
    <published>2018-04-29T16:00:00.000Z</published>
    <updated>2018-05-20T14:31:06.147Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、-OpenCV基本操作"><a href="#一、-OpenCV基本操作" class="headerlink" title="一、 OpenCV基本操作"></a>一、 OpenCV基本操作</h1><p>OpenCV是一套使用C++编写的开源跨平台计算机视觉库，它提供了很简单的Python调用接口：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br></pre></td></tr></table></figure><h2 id="1-图像的输入输出"><a href="#1-图像的输入输出" class="headerlink" title="1. 图像的输入输出"></a>1. 图像的输入输出</h2><h3 id="1-imread"><a href="#1-imread" class="headerlink" title="(1) imread()"></a>(1) imread()</h3><p>使用imread()从文件中读入图像数据，其返回值是一个元素类型为unit8的三维数组（OpenCV中还提供了imwrite()函数用于写入图片数据）。</p><h3 id="2-GUI工具"><a href="#2-GUI工具" class="headerlink" title="(2) GUI工具"></a>(2) GUI工具</h3><p>OpenCV提供了一些简单的GUI工具函数，如namedWindow(name)用于创建名为name的窗口，imshow()用于在窗口中显示图像。</p><h3 id="3-waitKey"><a href="#3-waitKey" class="headerlink" title="(3) waitKey()"></a>(3) waitKey()</h3><p>watiKey(time)表示等待用户按键，如果time=0则表示永远等待。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">"Diablo.jpg"</span>)</span><br><span class="line">print(type(img),img.shape,img.dtype)</span><br><span class="line">cv2.namedWindow(<span class="string">"Diablo"</span>)</span><br><span class="line">cv2.imshow(<span class="string">"Diablo"</span>,img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;numpy.ndarray&#39;&gt; (1200, 1920, 3) uint8-1</code></pre><p><img src="/img/OpenCV简单用法/Diablo.jpg" alt="大菠萝"></p><h2 id="2-黑白转换"><a href="#2-黑白转换" class="headerlink" title="2. 黑白转换"></a>2. 黑白转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)</span><br><span class="line">print(img_gray.shape)</span><br><span class="line">cv2.namedWindow(<span class="string">"Diablo_gray"</span>)</span><br><span class="line">cv2.imshow(<span class="string">"Diablo_gray"</span>,img_gray)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>(1200, 1920)-1</code></pre><p><img src="/img/OpenCV简单用法/Diablo.png" alt="黑白大菠萝"></p><h2 id="3-图像类型"><a href="#3-图像类型" class="headerlink" title="3. 图像类型"></a>3. 图像类型</h2><h3 id="通道"><a href="#通道" class="headerlink" title="通道"></a>通道</h3><p>每个图片都可以看成像素点矩阵，图像中的每个像素点可能有多个通道，即包含多个颜色成分，例如用单通道可以表示灰度图像，而使用红绿蓝三个通道可以表示彩色图像，用4个通道可以表示带透明度的彩色图像。</p><p>如上面的图像，原本的彩色图像的shape为(1200,1920,3)，转换成黑白图像之后形状就变成了(1200,1920）。</p><h3 id="比特"><a href="#比特" class="headerlink" title="比特"></a>比特</h3><p>比特数又叫像素深度或图像深度，是一个像素点所占的总位数。</p><p>常说的256色图就是指像素点的比特数为8的图片，可以表示$2^8=256$种颜色</p><p>图像转换工作可以在读取文件的时候完成，imread()的第二个参数有如下选择：</p><h3 id="（1）IMREAD-ANYCOLOR"><a href="#（1）IMREAD-ANYCOLOR" class="headerlink" title="（1）IMREAD_ANYCOLOR"></a>（1）IMREAD_ANYCOLOR</h3><p>转换成8比特的图像，通道数由图像文件决定，但是4通道的图像会被自动转换成三通道。</p><h3 id="（2）IMREAD-ANYDEPTH"><a href="#（2）IMREAD-ANYDEPTH" class="headerlink" title="（2）IMREAD_ANYDEPTH"></a>（2）IMREAD_ANYDEPTH</h3><p>转换为单通道，比特数由图像文件决定。</p><h3 id="（3）IMREAD-COLOR"><a href="#（3）IMREAD-COLOR" class="headerlink" title="（3）IMREAD_COLOR"></a>（3）IMREAD_COLOR</h3><p>转换成三通道，8比特的图像</p><h3 id="（4）IMREAD-GRAYSCALE"><a href="#（4）IMREAD-GRAYSCALE" class="headerlink" title="（4）IMREAD_GRAYSCALE"></a>（4）IMREAD_GRAYSCALE</h3><p>转换成单通道，8比特的图像</p><h3 id="（5）IMREAD-UNCHANGED"><a href="#（5）IMREAD-UNCHANGED" class="headerlink" title="（5）IMREAD_UNCHANGED"></a>（5）IMREAD_UNCHANGED</h3><p>使用图像文件的通道数和比特数</p><h2 id="4-图像输出"><a href="#4-图像输出" class="headerlink" title="4. 图像输出"></a>4. 图像输出</h2><p>imwrite()将数组编码成指定的图像格式并写入文件，图像的格式由文件的扩展名决定。某些格式由额外的图像参数，例如JPEG格式的文件可以指定画质参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">"Diablo.jpg"</span>)</span><br><span class="line"><span class="keyword">for</span> quality <span class="keyword">in</span> [<span class="number">30</span>,<span class="number">60</span>,<span class="number">90</span>]:</span><br><span class="line">    cv2.imwrite(<span class="string">"Diablo%d.jpg"</span> % quality,img,[cv2.IMWRITE_JPEG_QUALITY,quality])</span><br></pre></td></tr></table></figure><h2 id="5-图像与字节序列的转换"><a href="#5-图像与字节序列的转换" class="headerlink" title="5. 图像与字节序列的转换"></a>5. 图像与字节序列的转换</h2><p>imdecode()可以把图像文件数据解码成图像数组，imencode()则可以把图像数组编码成图像文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"Diablo.jpg"</span>,<span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    jpg_str = f.read()</span><br><span class="line"></span><br><span class="line">jpg_data = np.frombuffer(jpg_str,np.uint8)</span><br><span class="line">img = cv2.imdecode(jpg_data,cv2.IMREAD_UNCHANGED)</span><br><span class="line">img</span><br></pre></td></tr></table></figure><pre><code>array([[[17, 19, 20],        [ 5,  7,  8],        [ 3,  5,  6],        ...,         [ 2,  8,  7],        [ 0,  0,  1],        [ 4,  9, 10]],       [[ 0,  2,  3],        [ 4,  6,  7],        [ 3,  5,  6],        ...,         [ 3,  1,  1],        [ 9,  6,  8],        [ 3,  0,  2]],       [[14, 16, 17],        [ 8, 10, 11],        [ 9, 11, 12],        ...,         [21, 19, 19],        [25, 22, 24],        [12,  9, 11]],       ...,        [[ 9, 28, 33],        [54, 63, 67],        [15, 18, 23],        ...,         [ 8, 34, 40],        [16, 19, 27],        [18, 33, 36]],       [[ 7, 23, 29],        [27, 54, 58],        [ 6, 34, 35],        ...,         [25, 49, 55],        [15, 25, 32],        [11, 26, 29]],       [[14,  4, 16],        [35, 57, 62],        [10, 46, 46],        ...,         [28, 46, 53],        [40, 56, 62],        [13, 25, 29]]], dtype=uint8)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image <span class="comment">#将图片嵌入Jupyter Notebook</span></span><br><span class="line">res,jpg_data = cv2.imencode(<span class="string">".jpg"</span>,img)</span><br><span class="line">jpg_str = jpg_data.tobytes()</span><br><span class="line">Image(data = jpg_str)</span><br></pre></td></tr></table></figure><p><img src="/img/OpenCV简单用法/output_15_0.jpeg" alt="jpeg"></p><h1 id="二、OpenCV图像处理"><a href="#二、OpenCV图像处理" class="headerlink" title="二、OpenCV图像处理"></a>二、OpenCV图像处理</h1><h2 id="1-二维卷积"><a href="#1-二维卷积" class="headerlink" title="1. 二维卷积"></a>1. 二维卷积</h2><p>图像卷积的概念请参考<a href="https://blog.csdn.net/chaipp0607/article/details/72236892?locationNum=9&amp;fps=1" target="_blank" rel="noopener">数字图像处理中的卷积</a>，常见的几种功能的卷积核有如下几种：</p><h3 id="1-模糊化"><a href="#1-模糊化" class="headerlink" title="(1)模糊化"></a>(1)模糊化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 平滑均值滤波卷积核</span></span><br><span class="line"></span><br><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">             [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">             [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 高斯平滑卷积核</span></span><br><span class="line"></span><br><span class="line">B = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">            [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">            [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]])</span><br></pre></td></tr></table></figure><p>使用高斯卷积核处理之后的图像：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line">%matplotlib inline</span><br><span class="line">kernel = B/<span class="number">14</span><span class="comment">#卷积核的所有元素之和最好为1，否则会出现过亮或者过暗的情况</span></span><br><span class="line">src = cv2.imread(<span class="string">"Diablo.jpg"</span>)</span><br><span class="line">dst = cv2.filter2D(src,<span class="number">-1</span>,kernel)</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">35</span>))</span><br><span class="line">plt.imshow(dst[:,:,::<span class="number">-1</span>])<span class="comment">#matplotlib 和opencv的颜色是相反的</span></span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0x10ae5e48&gt;</code></pre><p><img src="/img/OpenCV简单用法/output_22_1.png" alt="png"></p><p>……效果不太明显，多模糊几次试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kernel = B/<span class="number">14</span><span class="comment">#卷积核的所有元素之和最好为1，否则会出现过亮或者过暗的情况</span></span><br><span class="line">src = cv2.imread(<span class="string">"Diablo.jpg"</span>)</span><br><span class="line">dst = cv2.filter2D(src,<span class="number">-1</span>,kernel)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    dst = cv2.filter2D(dst,<span class="number">-1</span>,kernel)</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">35</span>))</span><br><span class="line">plt.imshow(dst[:,:,::<span class="number">-1</span>])</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0x119bf780&gt;</code></pre><p><img src="output_24_1.png" alt="png"></p><p>整个图片都朦胧起来了</p><h3 id="2-锐化"><a href="#2-锐化" class="headerlink" title="(2) 锐化"></a>(2) 锐化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">C = np.array([[<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>],</span><br><span class="line">             [<span class="number">-1</span>,<span class="number">9</span>,<span class="number">-1</span>],</span><br><span class="line">             [<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>]])</span><br><span class="line"></span><br><span class="line">kernel = C</span><br><span class="line">src = cv2.imread(<span class="string">"Diablo.jpg"</span>)</span><br><span class="line">dst = cv2.filter2D(src,<span class="number">-1</span>,kernel)</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">35</span>))</span><br><span class="line">plt.imshow(dst[:,:,::<span class="number">-1</span>])</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0xbdef7b8&gt;</code></pre><p><img src="/img/OpenCV简单用法/output_27_1.png" alt="png"></p><h3 id="3-边缘检测"><a href="#3-边缘检测" class="headerlink" title="(3) 边缘检测"></a>(3) 边缘检测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">D = np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">             [<span class="number">1</span>,<span class="number">-8</span>,<span class="number">1</span>],</span><br><span class="line">             [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">kernel = D</span><br><span class="line">src = cv2.imread(<span class="string">"Diablo.jpg"</span>)</span><br><span class="line">dst = cv2.filter2D(src,<span class="number">-1</span>,kernel)</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">35</span>))</span><br><span class="line">plt.imshow(dst[:,:,::<span class="number">-1</span>])</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0xc0e63c8&gt;</code></pre><p><img src="/img/OpenCV简单用法/output_29_1.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、-OpenCV基本操作&quot;&gt;&lt;a href=&quot;#一、-OpenCV基本操作&quot; class=&quot;headerlink&quot; title=&quot;一、 OpenCV基本操作&quot;&gt;&lt;/a&gt;一、 OpenCV基本操作&lt;/h1&gt;&lt;p&gt;OpenCV是一套使用C++编写的开源跨平台计算机视
      
    
    </summary>
    
    
      <category term="OpenCV" scheme="http://yoursite.com/tags/OpenCV/"/>
    
      <category term="图像处理" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>OpenCV简单用法（二）</title>
    <link href="http://yoursite.com/2018/04/30/OpenCV%E7%AE%80%E5%8D%95%E7%94%A8%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://yoursite.com/2018/04/30/OpenCV简单用法（二）/</id>
    <published>2018-04-29T16:00:00.000Z</published>
    <updated>2018-05-20T14:33:35.849Z</updated>
    
    <content type="html"><![CDATA[<h1 id="最简单的人脸识别"><a href="#最简单的人脸识别" class="headerlink" title="最简单的人脸识别"></a>最简单的人脸识别</h1><p>还是用那张大菠萝的图片，这里需要用到两个文件：haarcascade_frontalface_default.xml和haarcascade_eye.xml，一个是找人脸的，一个是找眼睛的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line">face_cascade = cv2.CascadeClassifier(<span class="string">r'G:\opencv\sources\data\haarcascades\haarcascade_frontalface_default.xml'</span>)</span><br><span class="line"></span><br><span class="line">eye_cascade = cv2.CascadeClassifier(<span class="string">r'G:\opencv\sources\data\haarcascades\haarcascade_eye.xml'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#cap = cv2.VideoCapture(1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#while True:</span></span><br><span class="line">img = cv2.imread(<span class="string">"H:/learning_notes/study/opencv/Diablo.jpg"</span>)</span><br><span class="line">gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)</span><br><span class="line">faces = face_cascade.detectMultiScale(gray,<span class="number">1.3</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> (x,y,w,h) <span class="keyword">in</span> faces:</span><br><span class="line">    cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">2</span>)<span class="comment">#找到脸之后画个长方形框出来</span></span><br><span class="line">    roi_gray = gray[y:y+h,x:x+w]</span><br><span class="line">    roi_color = img[y:y+h,x:x+w]</span><br><span class="line">    eyes = eye_cascade.detectMultiScale(roi_gray)<span class="comment">#在脸的范围找眼镜</span></span><br><span class="line">    <span class="keyword">for</span> (ex,ey,ew,eh) <span class="keyword">in</span> eyes:</span><br><span class="line">        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'img'</span>,img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#cap.release()</span></span><br><span class="line"><span class="comment">#cv2.destroyAllWindows()</span></span><br></pre></td></tr></table></figure><p>运行以下看看结果：</p><p><img src="/img/OpenCV简单用法/face.png" alt="识别结果"></p><p>只有圣骑士的脸被识别出来了，看来只有正面人脸才能识别出来，大菠萝虽然也是正脸，可惜长得太丑了。至于眼睛，受图片清晰度所限，是找不到了。</p><p>那么我们换一张年轻的战神的照片试试:</p><p><img src="/img/OpenCV简单用法/Atreus.jpg" alt="年轻的战神"></p><p>识别结果：</p><p><img src="/img/OpenCV简单用法/face_atreus.png" alt="戴眼镜的战神"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;最简单的人脸识别&quot;&gt;&lt;a href=&quot;#最简单的人脸识别&quot; class=&quot;headerlink&quot; title=&quot;最简单的人脸识别&quot;&gt;&lt;/a&gt;最简单的人脸识别&lt;/h1&gt;&lt;p&gt;还是用那张大菠萝的图片，这里需要用到两个文件：haarcascade_frontalface
      
    
    </summary>
    
    
      <category term="OpenCV" scheme="http://yoursite.com/tags/OpenCV/"/>
    
      <category term="图像处理" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>LSTM预测股票走势</title>
    <link href="http://yoursite.com/2018/04/20/LSTM%E9%A2%84%E6%B5%8B%E8%82%A1%E7%A5%A8%E8%B5%B0%E5%8A%BF/"/>
    <id>http://yoursite.com/2018/04/20/LSTM预测股票走势/</id>
    <published>2018-04-19T16:00:00.000Z</published>
    <updated>2018-05-20T14:34:35.041Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用LSTM预测股票趋势"><a href="#使用LSTM预测股票趋势" class="headerlink" title="使用LSTM预测股票趋势"></a>使用LSTM预测股票趋势</h1><p>参考上一篇利用LSTM预测美元汇率的文章，可以自行编写一个程序用于预测股票涨跌。</p><h2 id="1-工具"><a href="#1-工具" class="headerlink" title="1. 工具"></a>1. 工具</h2><h3 id="数据来源：Tushare"><a href="#数据来源：Tushare" class="headerlink" title="数据来源：Tushare"></a>数据来源：Tushare</h3><p>Tushare是一个开源财经数据包，其数据主要来自新浪财经和腾讯财经，调用起来非常方便。</p><h3 id="模型工具：TensorFlow"><a href="#模型工具：TensorFlow" class="headerlink" title="模型工具：TensorFlow"></a>模型工具：TensorFlow</h3><p>TensorFlow不用过多介绍，本文中采用了自己搭建LSTM神经元的方式构建神经网络，以便与上一篇文章相呼应。</p><h2 id="2-导入数据"><a href="#2-导入数据" class="headerlink" title="2. 导入数据"></a>2. 导入数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获取历史收盘价格并归一化</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#加载数据</span></span><br><span class="line">    stocks = ts.get_hist_data(code=<span class="string">'600848'</span>,start=<span class="string">'2010-01-01'</span>,end=<span class="string">'2017-12-31'</span>)</span><br><span class="line">    close_data = stocks[<span class="string">'close'</span>].values</span><br><span class="line">    <span class="comment">#归一化</span></span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    scaled_data = scaler.fit_transform(close_data.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> scaled_data</span><br></pre></td></tr></table></figure><h2 id="3-处理数据"><a href="#3-处理数据" class="headerlink" title="3. 处理数据"></a>3. 处理数据</h2><p>因为是做预测实验，所以需要将数据按时间分割，下面的代码中将数据分割成了每windowsize个x对应一个y。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">window_data</span><span class="params">(data,window_size)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    切割数据，长度比x:y = 7:1</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x = []</span><br><span class="line">    y = []</span><br><span class="line">    i = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">while</span> (i + window_size) &lt;= data.shape[<span class="number">0</span>] <span class="number">-1</span>:</span><br><span class="line">        x.append(data[i:i+window_size])</span><br><span class="line">        y.append(data[i+window_size])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> len(x) == len(y)</span><br><span class="line">    <span class="keyword">return</span> x,y</span><br></pre></td></tr></table></figure></p><p>分割完了之后要进行数据划分，划分成训练数据和测试数据，以便验证模型效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_test</span><span class="params">(x,y,test_size)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    划分训练与测试数据</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x_train = np.array(x[:int(len(x)*test_size)])</span><br><span class="line">    y_train = np.array(y[:int(len(x)*test_size)])</span><br><span class="line">    x_test = np.array(x[int(len(x)*test_size):])</span><br><span class="line">    y_test = np.array(y[int(len(x)*test_size):])</span><br><span class="line">    <span class="keyword">return</span> x_train,y_train,x_test,y_test</span><br></pre></td></tr></table></figure><h2 id="4-定义LSTM神经元"><a href="#4-定义LSTM神经元" class="headerlink" title="4. 定义LSTM神经元"></a>4. 定义LSTM神经元</h2><p>下面各个参数的具体功能可以参考<a href="https://arctanxy.github.io/2018/04/15/LSTM%E9%A2%84%E6%B5%8B%E6%B1%87%E7%8E%87%E5%8F%98%E5%8C%96/" target="_blank" rel="noopener">上一篇文章</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LSTM_cell</span><span class="params">(input,output,state)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义LSTM神经元</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#输入门</span></span><br><span class="line">    weights_input_gate = tf.Variable(tf.truncated_normal([<span class="number">1</span>,HIDDEN_LAYER],stddev=<span class="number">0.05</span>))<span class="comment">#tf.truncated_normal用于生成一定维度的正态分布数据</span></span><br><span class="line">    weights_input_hidden = tf.Variable(tf.truncated_normal([HIDDEN_LAYER,HIDDEN_LAYER],stddev=<span class="number">0.05</span>))</span><br><span class="line">    bias_input = tf.Variable(tf.zeros([HIDDEN_LAYER]))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#遗忘门</span></span><br><span class="line">    weights_forget_gate = tf.Variable(tf.truncated_normal([<span class="number">1</span>, HIDDEN_LAYER], stddev=<span class="number">0.05</span>))</span><br><span class="line">    weights_forget_hidden = tf.Variable(tf.truncated_normal([HIDDEN_LAYER, HIDDEN_LAYER], stddev=<span class="number">0.05</span>))</span><br><span class="line">    bias_forget = tf.Variable(tf.zeros([HIDDEN_LAYER]))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#输出门</span></span><br><span class="line">    weights_output_gate = tf.Variable(tf.truncated_normal([<span class="number">1</span>, HIDDEN_LAYER], stddev=<span class="number">0.05</span>))</span><br><span class="line">    weights_output_hidden = tf.Variable(tf.truncated_normal([HIDDEN_LAYER, HIDDEN_LAYER], stddev=<span class="number">0.05</span>))</span><br><span class="line">    bias_output = tf.Variable(tf.zeros([HIDDEN_LAYER]))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#记忆单元</span></span><br><span class="line">    weights_memory_cell = tf.Variable(tf.truncated_normal([<span class="number">1</span>, HIDDEN_LAYER], stddev=<span class="number">0.05</span>))</span><br><span class="line">    weights_memory_cell_hidden = tf.Variable(tf.truncated_normal([HIDDEN_LAYER, HIDDEN_LAYER], stddev=<span class="number">0.05</span>))</span><br><span class="line">    bias_memory_cell = tf.Variable(tf.zeros([HIDDEN_LAYER]))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#定义各个门与状态</span></span><br><span class="line">    input_gate = tf.sigmoid(tf.matmul(input, weights_input_gate) + tf.matmul(output, weights_input_hidden) + bias_input)</span><br><span class="line">    forget_gate = tf.sigmoid(tf.matmul(input, weights_forget_gate) + tf.matmul(output, weights_forget_hidden) + bias_forget)</span><br><span class="line">    output_gate = tf.sigmoid(tf.matmul(input, weights_output_gate) + tf.matmul(output, weights_output_hidden) + bias_output)</span><br><span class="line">    memory_cell = tf.tanh(tf.matmul(input, weights_memory_cell) + tf.matmul(output, weights_memory_cell_hidden) + bias_memory_cell)</span><br><span class="line">    state = state * forget_gate + input_gate * memory_cell</span><br><span class="line">    output = output_gate * tf.tanh(state)</span><br><span class="line">    <span class="keyword">return</span> state, output</span><br></pre></td></tr></table></figure><h2 id="5-训练模型"><a href="#5-训练模型" class="headerlink" title="5. 训练模型"></a>5. 训练模型</h2><h3 id="1-定义参数"><a href="#1-定义参数" class="headerlink" title="(1) 定义参数"></a>(1) 定义参数</h3><p>定义一些必须的模型参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">7</span><span class="comment">#模型每批次训练所接受的数据量</span></span><br><span class="line">WINDOW_SIZE = <span class="number">7</span><span class="comment">#滑窗法切割</span></span><br><span class="line">HIDDEN_LAYER = <span class="number">256</span><span class="comment">#隐藏层层数</span></span><br><span class="line">CLIP_MARGIN = <span class="number">4</span><span class="comment">#用于控制梯度范围的参数</span></span><br><span class="line">LEARNING_RATE = <span class="number">0.001</span><span class="comment">#步长</span></span><br><span class="line">EPOCHS = <span class="number">100</span><span class="comment">#迭代次数</span></span><br></pre></td></tr></table></figure><h3 id="2-定义变量"><a href="#2-定义变量" class="headerlink" title="(2) 定义变量"></a>(2) 定义变量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    data = get_data()</span><br><span class="line">    x,y = window_data(data,WINDOW_SIZE)</span><br><span class="line">    x_train,y_train,x_test,y_test = train_test(x,y,test_size=<span class="number">0.25</span>)</span><br><span class="line">    inputs = tf.placeholder(tf.float32,[BATCH_SIZE,WINDOW_SIZE,<span class="number">1</span>])</span><br><span class="line">    targets = tf.placeholder(tf.float32,[BATCH_SIZE,<span class="number">1</span>])</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="comment">#输出参数</span></span><br><span class="line">    weights_output = tf.Variable(tf.truncated_normal([HIDDEN_LAYER,<span class="number">1</span>],stddev=<span class="number">0.05</span>))</span><br><span class="line">    bias_output_layer = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(BATCH_SIZE):</span><br><span class="line">        batch_state = np.zeros([<span class="number">1</span>,HIDDEN_LAYER],dtype=np.float32)</span><br><span class="line">        batch_output = np.zeros([<span class="number">1</span>,HIDDEN_LAYER],dtype=np.float32)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(WINDOW_SIZE):</span><br><span class="line">            batch_state,batch_output = LSTM_cell(tf.reshape(inputs[i][j],(<span class="number">-1</span>,<span class="number">1</span>)),batch_state,batch_output)</span><br><span class="line">        outputs.append(tf.matmul(batch_output,weights_output) + bias_output_layer)</span><br><span class="line">    <span class="comment">#定义模型损失</span></span><br><span class="line">    losses = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(outputs)):</span><br><span class="line">        losses.append(tf.losses.mean_squared_error(tf.reshape(targets[i],(<span class="number">-1</span>,<span class="number">1</span>)),outputs[i]))</span><br><span class="line">    loss = tf.reduce_mean(losses)</span><br><span class="line">    <span class="comment">#定义优化器</span></span><br><span class="line">    gradients = tf.gradients(loss,tf.trainable_variables())<span class="comment">#计算梯度</span></span><br><span class="line">    clipped,_ = tf.clip_by_global_norm(gradients,CLIP_MARGIN)<span class="comment">#让梯度控制在一定范围内，防止梯度消失或者梯度爆炸</span></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(LEARNING_RATE)</span><br><span class="line">    trained_optimizer = optimizer.apply_gradients(zip(gradients,tf.trainable_variables()))</span><br></pre></td></tr></table></figure><h3 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="(3) 训练模型"></a>(3) 训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(EPOCHS):</span><br><span class="line">    trained_scores = []</span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    epoch_loss = []</span><br><span class="line">    <span class="keyword">while</span> (j+BATCH_SIZE) &lt;= len(x_train):</span><br><span class="line">        x_batch = x_train[j:j+BATCH_SIZE]</span><br><span class="line">        y_batch = y_train[j:j+BATCH_SIZE]</span><br><span class="line">        <span class="comment">#c为cost，o为output</span></span><br><span class="line">        o,c,_ = sess.run([outputs,loss,trained_optimizer],feed_dict=&#123;inputs:x_batch,targets:y_batch&#125;)</span><br><span class="line">        epoch_loss.append(c)</span><br><span class="line">        trained_scores.append(o)</span><br><span class="line">        j += BATCH_SIZE</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (i%<span class="number">30</span>) == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Loss:&#123;&#125;"</span>.format(np.mean(epoch_loss)))</span><br></pre></td></tr></table></figure><h2 id="6-检验模型"><a href="#6-检验模型" class="headerlink" title="6. 检验模型"></a>6. 检验模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#测试</span></span><br><span class="line">tests = []</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i+BATCH_SIZE &lt;= len(x_test):</span><br><span class="line">    o = sess.run([outputs],feed_dict=&#123;inputs:x_test[i:i+BATCH_SIZE]&#125;)</span><br><span class="line">    i += BATCH_SIZE</span><br><span class="line">    tests.append(o)</span><br><span class="line"><span class="comment">#因为得到的预测数据是一格一格的滑窗数据，有很多重复数据，需要进行处理</span></span><br><span class="line">tests_new = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(tests)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(tests[i][<span class="number">0</span>])):</span><br><span class="line">        tests_new.append(tests[i][<span class="number">0</span>][j])</span><br><span class="line"><span class="comment">#将结果一维化</span></span><br><span class="line">tests_new = np.squeeze(tests_new)</span><br><span class="line">print(len(data),len(tests_new))</span><br><span class="line">print(tests_new)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.plot(range(len(data)),data,color = <span class="string">'r'</span>)</span><br><span class="line">plt.plot(range(len(data)-len(tests_new),len(data)),tests_new,color = <span class="string">'g'</span>)</span><br><span class="line">plt.show()</span><br><span class="line">fig.savefig(<span class="string">"H:/learning_notes/study/ttfunds/prediction.jpg"</span>)</span><br></pre></td></tr></table></figure><p>最终预测结果为：</p><p><img src="https://github.com/Arctanxy/learning_notes/blob/master/study/ttfunds/prediction.jpg?raw=true" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;使用LSTM预测股票趋势&quot;&gt;&lt;a href=&quot;#使用LSTM预测股票趋势&quot; class=&quot;headerlink&quot; title=&quot;使用LSTM预测股票趋势&quot;&gt;&lt;/a&gt;使用LSTM预测股票趋势&lt;/h1&gt;&lt;p&gt;参考上一篇利用LSTM预测美元汇率的文章，可以自行编写一个程
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="时间序列" scheme="http://yoursite.com/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
      <category term="LSTM" scheme="http://yoursite.com/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯分类器的Python实现</title>
    <link href="http://yoursite.com/2018/04/17/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84Python%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2018/04/17/朴素贝叶斯分类器的Python实现/</id>
    <published>2018-04-16T16:00:00.000Z</published>
    <updated>2018-05-20T14:35:46.994Z</updated>
    
    <content type="html"><![CDATA[<h1 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h1><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h3><p>贝叶斯定理的公式很简单：</p><script type="math/tex; mode=display">P(A|B) = \frac{P(B|A)* P(A)}{P(B)}</script><p>常用于解决分类问题。</p><h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>中文名比较好听，叫朴素贝叶斯，英文叫Naive Bayes，Naive是什么意思大家都知道，朴素贝叶斯的朴素就体现在它假设所有的属性（即特征）之间相互独立，这一假设可以表述为：</p><script type="math/tex; mode=display">P(X|Y=y) = \prod_{i=1}^{d} P(X_i|Y = y)</script><p>这一以来，前面的贝叶斯定理就可以表述为：</p><script type="math/tex; mode=display">P(Y|X) = \frac{P(Y)\prod_{i=1}^{d}P(X_i|Y)}{P(X)}</script><p>使Y的条件概率最大的类别Y就是样本X所属的类别。</p><p>而对于每个样本来说，$P(X)$是不随标签$Y_i$改变的，所以，只需比较</p><script type="math/tex; mode=display">P(Y)\prod_{i=1}^{d}P(X_i|Y)</script><p>就可以了。</p><h2 id="Python代码实现"><a href="#Python代码实现" class="headerlink" title="Python代码实现"></a>Python代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">朴素贝叶斯模型</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载鸢尾花数据</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    data = load_iris()</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">'data'</span>],data[<span class="string">'target'</span>]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NBClassifier</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.y = []<span class="comment">#标签集合</span></span><br><span class="line">        self.x = []<span class="comment">#每个属性的数值集合</span></span><br><span class="line">        self.py = defaultdict(float)<span class="comment">#标签的概率分布</span></span><br><span class="line">        self.pxy = defaultdict(dict)<span class="comment">#每个标签下的每个属性的概率分布</span></span><br><span class="line">        self.n = <span class="number">5</span><span class="comment">#分级的级数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prob</span><span class="params">(self,element,arr)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算元素在列表中出现的频率</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        prob = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> arr:</span><br><span class="line">            <span class="keyword">if</span> element == a:</span><br><span class="line">                prob += <span class="number">1</span>/len(arr)</span><br><span class="line">        <span class="keyword">if</span> prob == <span class="number">0.0</span>:</span><br><span class="line">            prob = <span class="number">0.001</span></span><br><span class="line">        <span class="keyword">return</span> prob</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_set</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        self.y = list(set(y))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">            self.x.append(list(set(x[:,i])))<span class="comment">#记录下每一列的数值集</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        训练模型</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.preprocess(x)</span><br><span class="line">        self.get_set(x,y)</span><br><span class="line">        <span class="comment">#1. 获取p(y)</span></span><br><span class="line">        <span class="keyword">for</span> yi <span class="keyword">in</span> self.y:</span><br><span class="line">            self.py[yi] = self.prob(yi,y)</span><br><span class="line">        <span class="comment">#2. 获取p(x|y)</span></span><br><span class="line">        <span class="keyword">for</span> yi <span class="keyword">in</span> self.y:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">                sample = x[y==yi,i]<span class="comment">#标签yi下的样本</span></span><br><span class="line">                <span class="comment">#获取该列的概率分布</span></span><br><span class="line">                pxy = [self.prob(xi,sample) <span class="keyword">for</span> xi <span class="keyword">in</span> self.x[i]]</span><br><span class="line">                self.pxy[yi][i] = pxy</span><br><span class="line">        print(<span class="string">"train score"</span>,self.score(x,y))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_one</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        预测单个样本</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        max_prob = <span class="number">0.0</span></span><br><span class="line">        max_yi = self.y[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> yi <span class="keyword">in</span> self.y:</span><br><span class="line">            prob_y = self.py[yi]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">                prob_x_y = self.pxy[yi][i][self.x[i].index(x[i])]<span class="comment">#p(xi|y)</span></span><br><span class="line">                prob_y *= prob_x_y<span class="comment">#计算p(x1|y)p(x2|y)...p(xn|y)p(y)</span></span><br><span class="line">            <span class="keyword">if</span> prob_y &gt; max_prob:</span><br><span class="line">                max_prob = prob_y</span><br><span class="line">                max_yi = yi</span><br><span class="line">        <span class="keyword">return</span> max_yi</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,samples)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        预测函数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        samples = self.preprocess(samples)</span><br><span class="line">        y_list = []</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> range(samples.shape[<span class="number">0</span>]):</span><br><span class="line">            yi = self.predict_one(samples[m,:])</span><br><span class="line">            y_list.append(yi)</span><br><span class="line">        <span class="keyword">return</span> np.array(y_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        因为不同特征的数值集大小相差巨大，造成部分概率矩阵变得稀疏，需要进行数据分割</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">            x[:,i] = self.step(x[:,i],self.n)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self,arr,n)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        分为n阶</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        ma = max(arr)</span><br><span class="line">        mi = min(arr)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">                a = mi + (ma-mi)*(j/n)</span><br><span class="line">                b = mi + (ma-mi)*((j+<span class="number">1</span>)/n)</span><br><span class="line">                <span class="keyword">if</span> arr[i] &gt;= a <span class="keyword">and</span> arr[i] &lt;= b:</span><br><span class="line">                    arr[i] = j+<span class="number">1</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> arr</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        y_test = self.predict(x)</span><br><span class="line">        score = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(y)):</span><br><span class="line">            <span class="keyword">if</span> y_test[i] == y[i]:</span><br><span class="line">                score += <span class="number">1</span>/len(y)</span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    x,y = load_data()</span><br><span class="line">    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = <span class="number">0.5</span>,random_state = <span class="number">100</span>)</span><br><span class="line">    clf = NBClassifier()</span><br><span class="line">    clf.fit(x_train,y_train)</span><br><span class="line">    score = clf.score(x_test,y_test)</span><br><span class="line">    print(<span class="string">'test score'</span>,score)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;朴素贝叶斯算法&quot;&gt;&lt;a href=&quot;#朴素贝叶斯算法&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯算法&quot;&gt;&lt;/a&gt;朴素贝叶斯算法&lt;/h1&gt;&lt;h2 id=&quot;算法&quot;&gt;&lt;a href=&quot;#算法&quot; class=&quot;headerlink&quot; title=&quot;算
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="贝叶斯" scheme="http://yoursite.com/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
  </entry>
  
  <entry>
    <title>极大似然和最小二乘</title>
    <link href="http://yoursite.com/2018/04/17/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%92%8C%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98/"/>
    <id>http://yoursite.com/2018/04/17/极大似然和最小二乘/</id>
    <published>2018-04-16T16:00:00.000Z</published>
    <updated>2018-05-20T14:31:29.428Z</updated>
    
    <content type="html"><![CDATA[<h1 id="极大似然和最小二乘"><a href="#极大似然和最小二乘" class="headerlink" title="极大似然和最小二乘"></a>极大似然和最小二乘</h1><p>本文为PRML第三章3.1.1笔记，书中推导步骤过于简单，对长期没有接触高数的读者很不友好，故在此记录一下由极大似然到最小二乘的详细的推导过程。</p><p>线性回归的方程可以写成如下形式：</p><script type="math/tex; mode=display">y = f(x,w) + \epsilon</script><p>其中$\epsilon$是一个均值为零的高斯随机变量，精度（方差的倒数）为$\beta$。</p><p>可以认为$y$值符合均值为$f(x,w)$，方差为$\beta^{-1}$的高斯分布：</p><script type="math/tex; mode=display">p(y|x,w,\beta) = \mathcal N(y|f(x,w),\beta^{-1})</script><p>根据高斯分布的概率分布公式可以转化为：</p><script type="math/tex; mode=display">f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} = \sqrt{\frac{\beta}{2\pi}} e^{-\frac{\beta(x-\mu)^2}{2}}</script><script type="math/tex; mode=display">\mathcal N(y|f(x,w),\beta^{-1}) = \sqrt{\frac{\beta}{2\pi}} e^{-\frac{\beta(y-f(x,w))^2}{2}}</script><p>在此条件分布下，可以很快地求得$y$的条件均值：</p><script type="math/tex; mode=display">E[y|x] = \int yp(y|x)dy = f(x,w)</script><p>在实际问题中，$X$为数据集${x_1,…,x_n}$，对应的目标值$y_1,…y_n$，${y_n}$是一个列向量，记作$y$，可以得到如下公式：</p><script type="math/tex; mode=display">p(y|X,w,\beta) = \prod_{n=1}^{N} \mathcal N(y_n|w^T\phi(x_n),\beta^{-1})</script><p>对两边同时取导数</p><script type="math/tex; mode=display">ln p(y|X,w,\beta) = \sum_{n=1}^{N}ln \mathcal N(y_n|w^T\phi(x_n),\beta^{-1})</script><p>将前面的概率公式带入之后可以写成</p><script type="math/tex; mode=display">ln p(y|X,w,\beta) = \sum_{n=1}^{N}ln \{\sqrt{\frac{\beta}{2\pi}} e^{-\frac{\beta(y-f(x,w))^2}{2}}\}</script><script type="math/tex; mode=display">ln p(y|X,w,\beta) = \sum_{n=1}^{N} \{\frac{ln \beta}{2} - \frac{ln2\pi}{2} - \frac{\beta (y_n-f(x,w))^2}{2}\}</script><script type="math/tex; mode=display">ln p(y|X,w,\beta) = \frac{Nln \beta}{2} - \frac{Nln2\pi}{2} -\sum_{n=1}^{N} \frac{\beta (y_n-f(x,w))^2}{2}</script><p>为了使概率$p(y|X,w,\beta)$最大，就需要使$\sum<em>{n=1}^{N} \frac{\beta (y_n-f(x,w))^2}{2}$最小，也就是使$\sum</em>{n=1}^{N} (y_n-f(x,w))^2$最小。</p><p>如此一来，又变成了一个最小二乘问题，对其求导后令导数为零，即可得到最终$w$的解(具体推导过程参考<a href="https://arctanxy.github.io/2018/04/05/%E5%B2%AD%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">线性回归与岭回归的数学推导</a>)：</p><script type="math/tex; mode=display">w = (X^TX)^{-1}y^T X</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;极大似然和最小二乘&quot;&gt;&lt;a href=&quot;#极大似然和最小二乘&quot; class=&quot;headerlink&quot; title=&quot;极大似然和最小二乘&quot;&gt;&lt;/a&gt;极大似然和最小二乘&lt;/h1&gt;&lt;p&gt;本文为PRML第三章3.1.1笔记，书中推导步骤过于简单，对长期没有接触高数的读者很
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="http://yoursite.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>使用循环神经网络预测汇率涨跌</title>
    <link href="http://yoursite.com/2018/04/15/LSTM%E9%A2%84%E6%B5%8B%E6%B1%87%E7%8E%87%E5%8F%98%E5%8C%96/"/>
    <id>http://yoursite.com/2018/04/15/LSTM预测汇率变化/</id>
    <published>2018-04-14T16:00:00.000Z</published>
    <updated>2018-05-20T14:30:53.514Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用循环神经网络预测汇率涨跌"><a href="#使用循环神经网络预测汇率涨跌" class="headerlink" title="使用循环神经网络预测汇率涨跌"></a>使用循环神经网络预测汇率涨跌</h1><p>本文中讲简单地介绍如何使用时间序列分析的方法预测汇率变化。</p><h2 id="序列问题"><a href="#序列问题" class="headerlink" title="序列问题"></a>序列问题</h2><p>首先介绍一下序列问题，常见的机器学习问题都是是一对一的模型，如下图所示：</p><p><img src="https://cdn-images-1.medium.com/max/1600/0*7AIMLPm1e7hgGolz." alt="一对一模型"></p><p>在这个例子中，我们将一个输入数据传入到模型中，然后模型会根据传入数据生成一个结果，像线性回归，分类问题甚至图像分类卷积神经网络都属于这种类型。</p><p>这种模式经过修改可以用于处理一对多模型，如下图所示，模型的输出数据会作为新的输入数据传入回神经网络中，从而产生一系列的值，这种神经网络叫做循环神经网络。</p><p><img src="https://cdn-images-1.medium.com/max/1600/0*QFWZFOLMH4EyyZxu." alt="循环神经网络"></p><p>对于序列型的输入数据，循环神经网络的工作方式如下图所示，每个循环网络神经元的输出都会进入下一个神经元，作为下一个神经元的一部分输入数据：</p><p><img src="https://cdn-images-1.medium.com/max/1600/0*x1vmPLhmSow0kzvK." alt="循环神经网络处理序列问题"></p><p>上面的网络中的每个神经元都使用同一个公式：</p><script type="math/tex; mode=display">Y_t = tanh(wY_{t-1} + ux_t)</script><p>其中$Y<em>t$是当前神经元的输出，$Y</em>{t-1}$是上一个神经元的输出数据，$x_t$是当前神经元的原始输入，$w$和$u$都是权重参数。</p><p>可以通过简单地堆叠神经元来构建一个深层循环神经网络，但是简单的循环神经网络只能处理短时间记忆，对于需要长时间记忆的问题准确度会下降。</p><p>对于需要长时间依赖的序列分析问题，我们可以使用lstm神经网络来处理。</p><h2 id="LSTM神经网络简介"><a href="#LSTM神经网络简介" class="headerlink" title="LSTM神经网络简介"></a>LSTM神经网络简介</h2><p>在上世纪九十年代，<a href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735" target="_blank" rel="noopener">Sepp Hochreiter和Jurgen Schmidhuber</a>提出了LSTM神经网络，解决了传统的循环神经网络、隐马尔可夫模型和其他序列模型对长时间跨度不敏感的问题。</p><p><img src="https://cdn-images-1.medium.com/max/800/0*_rC7UKSazzfOkpFZ." alt="LSTM神经元内部结构"></p><p>LSTM神经元在传统循环神经网络中添加了一些逻辑门，它们的功能如下：</p><h3 id="1-遗忘门"><a href="#1-遗忘门" class="headerlink" title="1. 遗忘门"></a>1. 遗忘门</h3><script type="math/tex; mode=display">f_t = \sigma(W_f[h_{t-1},x_t]+b_f)</script><p>遗忘门会接受来自上一神经元的输出$h_{t-1}$以及当前神经元的输入$x_t$，通过线性变换之后传入sigmod函数，得到一个介于0到1的数字，这个数字可以认为是门的开度。这个数字会与内部状态相乘，所以这个门成为遗忘门，因为如果$f_t$为0时，当前的内部状态会被完全丢弃，如果$f_t$为1时，当前状态会保持完好无损。</p><h3 id="2-输入门"><a href="#2-输入门" class="headerlink" title="2. 输入门"></a>2. 输入门</h3><script type="math/tex; mode=display">i_t = \sigma(W_i[h_{t-1},x_t]+b_i</script><p>输入门中同样会取上一神经元的输出$h_{t-1}$以及当前神经元的输入$x_t$，经过线性变换后传入sigmoid函数，返回介于0到1到值，该值会与记忆单元的输出值相乘，记忆单元的方程如下：</p><script type="math/tex; mode=display">C_t = tanh(W_c[h_{t-1},x_t]+b_c</script><p>这一层会对当前输入和上一层输入的线性结果进行双曲正切函数处理，其返回的向量将会被添加到内部状态中。</p><p>内部状态通过如下规则更新：</p><script type="math/tex; mode=display">C_t = f_t * C_{t-1} + i_t * C_t</script><p>上一层的状态$C_{t-1}$会与遗忘门的返回值相乘，当前的$C_t$的$i_t$倍相加，得到该神经元的最终状态$C_t$，$C_t$会传输到输出门进行计算。</p><h3 id="3-输出门"><a href="#3-输出门" class="headerlink" title="3. 输出门"></a>3. 输出门</h3><script type="math/tex; mode=display">O_t = \sigma(W_o[h_{t-1},x_t] + b_o)</script><script type="math/tex; mode=display">h_t = O_t * tanh(C_t)</script><p>输出门决定了传入最终output的内部状态的比例，作用方式与前面两种门相似。</p><p>以上的三个们分别有独自的权重和偏置，也就是说神经网络将会学习保留多少往期数据，保留多少输入数据，传输多少内部状态。</p><p>在循环神经网络中，你不止向网络传入数据，还需要传入上一时刻的状态，这种特性对于需要联系上文的自然语言处理非常有效。</p><p>另外循环神经网络还可以用在时间序列分析、视频处理、语音识别等领域。</p><p>本文中将简单地介绍一下循环神经网络在汇率预测方面的应用。</p><p>将使用的数据是美元与印度卢比的在1980年1月2日到2017年8月10日这段时间内的汇率，共有13730条数据，图像如下：</p><p><img src="https://cdn-images-1.medium.com/max/800/0*UYHLdtUFPTM7YPs6." alt="汇率变化"></p><h2 id="模型搭建"><a href="#模型搭建" class="headerlink" title="模型搭建"></a>模型搭建</h2><h3 id="训练测试数据划分"><a href="#训练测试数据划分" class="headerlink" title="训练测试数据划分"></a>训练测试数据划分</h3><p>以1980/1/2到2009/12/31期间为训练数据，2010/1/1到2017/8/10期间为测试数据。</p><p><img src="https://cdn-images-1.medium.com/max/800/0*jXH_D2Zd8TOmXa1H." alt="数据划分"></p><p>分组之前要对数据进行归一化。</p><h3 id="尝试全连接神经网络"><a href="#尝试全连接神经网络" class="headerlink" title="尝试全连接神经网络"></a>尝试全连接神经网络</h3><p>所搭建的模型详情如下：<br><img src="https://cdn-images-1.medium.com/max/1600/0*u3xLjEmM4m-0Ucjr." alt="模型简介"></p><p>迭代200遍之后的预测结果如图所示：</p><p><img src="https://cdn-images-1.medium.com/max/1200/0*6-fJhYPOGwCzGEs7." alt="预测结果"></p><p>很明显预测结果与实际情况有较大的偏差。</p><h3 id="使用LSTM神经网络预测"><a href="#使用LSTM神经网络预测" class="headerlink" title="使用LSTM神经网络预测"></a>使用LSTM神经网络预测</h3><p>接下来改用LSTM神经网络进行建模。模型详情如下：<br><img src="https://cdn-images-1.medium.com/max/1600/0*fDevZBB0iBwHtlIw." alt="模型简介"></p><p><img src="https://cdn-images-1.medium.com/max/1200/1*ysQ--yj7je3GReiiX5knBg.png" alt="预测结果"></p><p>结果与实际曲线基本吻合，表明LSTM神经网络有应用于汇率预测的前景。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>LSTM模型在处理有时间依赖的序列数据时能获得较好的效果，常用于语音识别，音乐分析，手写识别，甚至人类行为分析，是一种拥有记忆能力，能模拟人类决策方式的模型。</p><p>本文译自：<br><a href="https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f" target="_blank" rel="noopener">https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f</a></p><blockquote><p>据原博客评论中所言，此博文可能是该博主非法转载，真正原文应该是：<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;使用循环神经网络预测汇率涨跌&quot;&gt;&lt;a href=&quot;#使用循环神经网络预测汇率涨跌&quot; class=&quot;headerlink&quot; title=&quot;使用循环神经网络预测汇率涨跌&quot;&gt;&lt;/a&gt;使用循环神经网络预测汇率涨跌&lt;/h1&gt;&lt;p&gt;本文中讲简单地介绍如何使用时间序列分析的方法
      
    
    </summary>
    
    
      <category term="LSTM" scheme="http://yoursite.com/tags/LSTM/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Random Lasso</title>
    <link href="http://yoursite.com/2018/04/08/Random%20Lasso/"/>
    <id>http://yoursite.com/2018/04/08/Random Lasso/</id>
    <published>2018-04-07T16:00:00.000Z</published>
    <updated>2018-05-20T14:31:12.765Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Random-Lasso"><a href="#Random-Lasso" class="headerlink" title="Random Lasso"></a>Random Lasso</h1><p>Random Lasso是一种较为容易实现的集成线性回归算法，其思路与随机森林极为相似。</p><h2 id="模型思路如下："><a href="#模型思路如下：" class="headerlink" title="模型思路如下："></a>模型思路如下：</h2><ol><li>从总样本中通过自助取样和随机采取特征的方式，得到N个样本（取样方式与随机森林的取样方式相同）；</li><li>对着N个样本分别建立N个Lasso回归模型，根据模型中的系数，以计算均值的方式确定每个特征的权重；</li><li>重新取样，步骤同1，但是特征取样的概率遵循特征权重；</li><li>得到N个样本后，对每个建立一个Adaptive Lasso模型，以所有模型预测结果的均值为最终结果。</li></ol><h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><h3 id="1-模型简介"><a href="#1-模型简介" class="headerlink" title="1. 模型简介"></a>1. 模型简介</h3><p>假设有样本：$(X<em>1,y_1),(X_2,y_2),…,(X_i,y_i),…,(X_n,y_n)$，其中$X_i = (x</em>{i1},…,x_{ip})^T$是一个p维向量，我们可以建立以下模型：</p><script type="math/tex; mode=display">y_i = \beta_1 x_i1 + ... + \beta_p x_{ip} + \epsilon</script><p>其中的$\epsilon$是一个符合$N(0,1)$正态分布的常数，</p><p>Lasso回归与普通线性回归不同的是它在常规的误差函数中添加了一个L1范数作为惩罚项，以减小模型发生过拟合的可能性，同时因为L1范数的特性，Lasso也具备了特征筛选的能力，而这个特征筛选的能力既是优点，也是缺点。Lasso回归中需要优化的误差项如下：</p><script type="math/tex; mode=display">\min_{\beta}\sum_{i=1}^{n}(y_i - \sum_{j=1}^{p}\beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p}\left|\beta_j\right|</script><p>Lasso非常好用，至今还有不少学者在研究Lasso，因为Lasso还有不少的可优化空间，在实际使用中，Lasso主要有两大局限<a href="https://mathscinet.ams.org/mathscinet-getitem?mr=2137327" target="_blank" rel="noopener">Zou</a>：</p><ol><li><p>如果模型中有一些高度线性相关的特征，Lasso通常会从中选择一个或者一部分特征，而其他的线性相关特征都置为0。这样的特性在某些应用场景中可能会造成不好的影响。比如在基因序列分析中，基因的表达水平都会遵循一个共同的生物途径，所以往往会体现出高度相关性，但是这些基因又都能作用于生物过程，Lasso却只会从中选择一两个基因。理想的模型应该能选择所有的重要基因，去除无关紧要的基因。</p></li><li><p>当p&gt;n时，即特征数量大于样本数量时，Lasso最多只能选择n个特征。这样的特性在某些场景同样会出现问题，比如上面的基因分析问题，可能会导致最终的特征选择不足，很多有用的特征无法取到。</p></li></ol><p>&lt;文中还提到了弹性网络，但是因为与Random Lasso模型不太相关，我就略过了&gt;</p><p><a href="https://mathscinet.ams.org/mathscinet-getitem?mr=2279469" target="_blank" rel="noopener">Zou</a>对Lasso进行了修改，对每个特征使用不同的惩罚项，形成了新的自适应Lasso回归，其误差项如下：</p><script type="math/tex; mode=display">\min_{\beta}\sum_{i=1}^{n}(y_i - \sum_{j=1}^{p}\beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} w_j \left|\beta_j\right|</script><p>其中的$w_j = \left| \hat{\beta}_j^ols \right|^{-r}$，$\hat{\beta}_j^ols$是$\beta_j$的最小二乘估计量，而r是一个正实数。</p><p>关于Adaptive Lasso的原理可以参看,<a href="http://www.oalib.com/paper/4762685#.Wslud_luYdU" target="_blank" rel="noopener">李锋，部分线性模型的AdaptiveLasso特征选择</a>。<a href="https://wenku.baidu.com/view/436f16e8dd36a32d7275819f.html" target="_blank" rel="noopener">基于迭代加权L1范数的稀疏阵列综合</a></p><blockquote><p>在Python 的sklearn库中，虽然没有adaptive lasso回归函数，但是却可以借助lasso实现adaptive lasso。具体实现将在下一篇文章中介绍。</p></blockquote><h3 id="2-RandomLasso"><a href="#2-RandomLasso" class="headerlink" title="2. RandomLasso"></a>2. RandomLasso</h3><p>了解了以上知识后，下面介绍Random Lasso模型的具体步骤。</p><p>前面提到了Lasso能够筛选特征，但是这种筛选特征的方式有一个短板，就是对于一些有线性相关的重要特征，Lasso只会从中挑选一个或者一部分。</p><p>如果一些独立的数据都符合相同的分布，我们希望Lasso能够从不同的数据集中选取出这些重要因素的不同子集，通过随机选择特征集的方式，我们最终选择的重要特征集合就有可能会包含大部分甚至全部的有线性相关的重要特征。同时这种方式也解决了Lasso的第二个局限。</p><p>RandomLasso的具体实现步骤如下</p><h4 id="生成特征权重"><a href="#生成特征权重" class="headerlink" title="生成特征权重"></a>生成特征权重</h4><ol><li>通过bootstrap自助取样采集B个大小为n的样本集；</li><li>从每个样本集中随机抽取一定比例的特征，利用这些特征生成新样本集；</li><li>对新样本集建立Lasso模型，并得到每个模型中的系数权重；</li><li>计算每个特征的平均系数权重。</li></ol><p>因为在每个Lasso模型中，重要因素的权重都会比较大，而不重要因素的权重会很小或者符号相反或者为零，所以使用平均系数就可以衡量出一个特征的重要性。</p><h4 id="选择特征"><a href="#选择特征" class="headerlink" title="选择特征"></a>选择特征</h4><ol><li>重新自助取样得到B个大小为n的样本集；</li><li>根据上一步中得到的系数权重进行特征抽取，抽取一定比例的特征，利用这些特征生成新样本集；</li><li>对新样本集建立Adaptive Lasso（或者Lasso）模型，并得到每个模型的系数权重；</li><li>计算每个特征的平均系数权重，得到最终的线性回归方程。</li></ol><p>在第3步中如果选择了Lasso模型的话，因素权重$w_j$有如下几种选择：</p><script type="math/tex; mode=display">w_j = 1/\left|\hat(\beta)_j^ols\right|^r</script><script type="math/tex; mode=display">w_j = 1/\left|\hat(\beta)_j^ridge\right|^r</script><script type="math/tex; mode=display">w_j = 1/\left|\hat(\beta)_j^uni\right|^r</script><p>$\hat(\beta)_j^ols$是$\beta_j$的普通最小二乘估计，$\hat(\beta)_j^ridge$是$\beta_j$的岭回归估计，$\hat(\beta)_j^uni$是$\beta_j$的单变量估计。r是一个正实数。我们的模型直接使用了前面生成的特征权重作为Adaptive Lasso的权重，模型表现较好。</p><p>在构建模型的过程中，需要选择样本集的个数B，和选取特征的比例q。其中B表现出来的效果是越大越好，而q可以通过交叉检验的效果来确定较好的q值，因为使用的自助取样，所以会有未取到的袋外数据，正好可以用来做交叉验证。</p><p>&lt; 后文是作者展示模型效果，有兴趣的读者请自行阅读&gt;</p><p>本文内容主要来自<a href="https://arxiv.org/pdf/1104.3398.pdf" target="_blank" rel="noopener">Random Lasso</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Random-Lasso&quot;&gt;&lt;a href=&quot;#Random-Lasso&quot; class=&quot;headerlink&quot; title=&quot;Random Lasso&quot;&gt;&lt;/a&gt;Random Lasso&lt;/h1&gt;&lt;p&gt;Random Lasso是一种较为容易实现的集成线性回归算法
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="回归" scheme="http://yoursite.com/tags/%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>线性回归中的梯度下降与一维搜索</title>
    <link href="http://yoursite.com/2018/04/07/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E7%BA%BF%E6%90%9C%E7%B4%A2/"/>
    <id>http://yoursite.com/2018/04/07/梯度下降与线搜索/</id>
    <published>2018-04-06T16:00:00.000Z</published>
    <updated>2018-05-20T14:31:32.660Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性回归中的梯度下降与一维搜索"><a href="#线性回归中的梯度下降与一维搜索" class="headerlink" title="线性回归中的梯度下降与一维搜索"></a>线性回归中的梯度下降与一维搜索</h1><p>之前讲到了一般线性回归和岭回归的矩阵求解方式，但是并非所有的模型都能方便地求出数学最优解，往往需要采取逐步迭代的方式寻找近似最优解。常见的方法有梯度下降法、牛顿迭代法、拟牛顿法等。为了便于直观地展现求解过程，本文中将以求二次函数的最小值为例，讲解梯度下降法的使用方法。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>假设有函数$y = 2x^2+ 3x +4$，绘制出的函数图像如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = np.linspace(<span class="number">-22</span>,<span class="number">20</span>,<span class="number">100</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = <span class="number">2</span>*x**<span class="number">2</span> + <span class="number">3</span>*x + <span class="number">4</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line">y = func(x)</span><br><span class="line">plt.plot(x,y,color = <span class="string">'g'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x86b9a90&gt;]</code></pre><p><img src="/img/output_3_1.png" alt="png"></p><p>可以很容易的得到y的导数方程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span>*x + <span class="number">3</span></span><br></pre></td></tr></table></figure><p>给定任意初始值x_0，设置步长为0.2，根据梯度下降原理，对最优解进行搜索</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimize</span><span class="params">(x0 = <span class="number">10</span>,step = <span class="number">0.2</span>)</span>:</span></span><br><span class="line">    x = x0</span><br><span class="line">    path = []</span><br><span class="line">    path.append(x)</span><br><span class="line">    <span class="comment">#先迭代五次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        x = x - step*gradient(x)</span><br><span class="line">        print(x)</span><br><span class="line">        path.append(x)</span><br><span class="line">    <span class="keyword">return</span> path</span><br><span class="line">path = minimize()</span><br><span class="line">plt.plot(x,y,color = <span class="string">'g'</span>)</span><br><span class="line">plt.plot(path,[func(x) <span class="keyword">for</span> x <span class="keyword">in</span> path],color = <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>1.4000000000000004-0.32000000000000006-0.664-0.7328-0.74656-0.749312-0.7498624-0.74997248-0.749994496-0.7499988992[&lt;matplotlib.lines.Line2D at 0x89bc278&gt;]</code></pre><p><img src="/img/梯度下降与线搜索/output_7_2.png" alt="png"></p><p>如果换一个x0效果会如何呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">path = minimize(<span class="number">20</span>)</span><br><span class="line">plt.plot(x,y,color = <span class="string">'g'</span>)</span><br><span class="line">plt.plot(path,[func(x) <span class="keyword">for</span> x <span class="keyword">in</span> path],color = <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>3.39999999999999860.07999999999999963-0.5840000000000001-0.7168-0.74336-0.748672-0.7497344-0.74994688-0.749989376-0.7499978752[&lt;matplotlib.lines.Line2D at 0x568a2b0&gt;]</code></pre><p><img src="/img/梯度下降与线搜索/output_9_2.png" alt="png"></p><p>其实对于这种单峰函数来说，初始值对最后结果的影响并不大，只是会影响迭代的次数而已。</p><p>我们再换增大一点步长试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">path = minimize(<span class="number">10</span>,<span class="number">0.6</span>)</span><br><span class="line">x = np.linspace(min(path),max(path),<span class="number">100</span>)</span><br><span class="line">plt.plot(x,[func(a) <span class="keyword">for</span> a <span class="keyword">in</span> x],color = <span class="string">'g'</span>)</span><br><span class="line">plt.plot(path,[func(b) <span class="keyword">for</span> b <span class="keyword">in</span> path],color = <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>-15.820.319999999999997-30.24799999999999440.54719999999999-58.56607999999998580.19251199999998-114.06951679999996157.89732351999993-222.8562529279999310.19875409919985[&lt;matplotlib.lines.Line2D at 0x8b9a1d0&gt;]</code></pre><p><img src="/img/梯度下降与线搜索/output_11_2.png" alt="png"></p><p>从图中可以看到，在步长设置不合理情况下，x有可能会逐渐偏离函数最小值，最终得到的函数值的绝对值越来越大，很快便会超出实数范围，得到inf。</p><p>为了解决这个问题，需要动态调整步长。</p><p>比较简单的思路是这样的：在每一步迭代之前检查一下y值是不是在增大，如果y值增大了，就缩小step的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimize</span><span class="params">(x0 = <span class="number">10</span>,step = <span class="number">0.2</span>,beta = <span class="number">0.8</span>)</span>:</span></span><br><span class="line">    x = x0</span><br><span class="line">    path = []</span><br><span class="line">    path.append(x)</span><br><span class="line">    <span class="comment">#先迭代五次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        new_x = x - step*gradient(x)</span><br><span class="line">        <span class="keyword">while</span> func(x) &lt; func(new_x):</span><br><span class="line">            step *= beta</span><br><span class="line">            new_x = x-step*gradient(x)</span><br><span class="line">        x = new_x</span><br><span class="line">        path.append(x)</span><br><span class="line">    <span class="keyword">return</span> path</span><br><span class="line">path = minimize(<span class="number">10</span>,<span class="number">0.6</span>)</span><br><span class="line">m = np.linspace(min(path),max(path),<span class="number">100</span>)</span><br><span class="line">plt.plot(m,[func(a) <span class="keyword">for</span> a <span class="keyword">in</span> m],color = <span class="string">'g'</span>)</span><br><span class="line">plt.plot(path,[func(b) <span class="keyword">for</span> b <span class="keyword">in</span> path],color = <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x8bd4da0&gt;]</code></pre><p><img src="/img/梯度下降与线搜索/output_14_1.png" alt="png"></p><p>小小的改动之后就可以避免函数增大，如果希望收敛速度更快一些的话，就需要调节beta，调节beta的难度比调节step简单得多。</p><p>如果想加快收敛速度的话，可以改用更加严谨的一维搜索方法。</p><h2 id="一维搜索"><a href="#一维搜索" class="headerlink" title="一维搜索"></a>一维搜索</h2><p>在同样的beta条件下，一维搜索能够更快地收敛，并且能step变得太小。</p><p>一维搜索有很多种实现方式，上面用的这种算是效率较高的一维搜索算法，名为：回溯线搜索 backtracking line search。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">line_search</span><span class="params">(x,gradient,step,threshold = <span class="number">0.1</span>,beta = <span class="number">0.8</span>)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> func(x-step*gradient) &gt; func(x) - threshold * step * gradient**<span class="number">2</span>:</span><br><span class="line">        step *= beta</span><br><span class="line">    <span class="keyword">return</span> step</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">line_minimize</span><span class="params">(x0 = <span class="number">10</span>,step = <span class="number">0.2</span>,beta = <span class="number">0.8</span>)</span>:</span></span><br><span class="line">    x = x0</span><br><span class="line">    path = []</span><br><span class="line">    path.append(x)</span><br><span class="line">    <span class="comment">#先迭代五次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        step = line_search(x,gradient(x),step)</span><br><span class="line">        x = x - step * gradient(x)</span><br><span class="line">        path.append(x)</span><br><span class="line">    <span class="keyword">return</span> path</span><br><span class="line"></span><br><span class="line">path = line_minimize(<span class="number">10</span>,<span class="number">0.6</span>)</span><br><span class="line">m = np.linspace(min(path),max(path),<span class="number">100</span>)</span><br><span class="line">plt.plot(m,[func(a) <span class="keyword">for</span> a <span class="keyword">in</span> m],color = <span class="string">'g'</span>)</span><br><span class="line">plt.plot(path,[func(b) <span class="keyword">for</span> b <span class="keyword">in</span> path],color = <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x8c33d68&gt;]</code></pre><p><img src="/img/梯度下降与线搜索/output_18_1.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;线性回归中的梯度下降与一维搜索&quot;&gt;&lt;a href=&quot;#线性回归中的梯度下降与一维搜索&quot; class=&quot;headerlink&quot; title=&quot;线性回归中的梯度下降与一维搜索&quot;&gt;&lt;/a&gt;线性回归中的梯度下降与一维搜索&lt;/h1&gt;&lt;p&gt;之前讲到了一般线性回归和岭回归的矩阵
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="回归" scheme="http://yoursite.com/tags/%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>线性回归的矩阵运算</title>
    <link href="http://yoursite.com/2018/04/05/Ridge/"/>
    <id>http://yoursite.com/2018/04/05/Ridge/</id>
    <published>2018-04-04T16:00:00.000Z</published>
    <updated>2018-05-20T14:31:16.837Z</updated>
    
    <content type="html"><![CDATA[<h1 id="岭回归的矩阵运算"><a href="#岭回归的矩阵运算" class="headerlink" title="岭回归的矩阵运算"></a>岭回归的矩阵运算</h1><h2 id="普通线性回归的矩阵运算"><a href="#普通线性回归的矩阵运算" class="headerlink" title="普通线性回归的矩阵运算"></a>普通线性回归的矩阵运算</h2><p>在训练数据<br>（1）各列数据线性独立<br>（2）样本数量多于特征数量<br>的前提下，可以使用矩阵形式计算线性回归的系数<br><a href="http://bourneli.github.io/linear-algebra/2016/03/03/linear-algebra-04-ATA-inverse.html" target="_blank" rel="noopener">参考</a></p><h3 id="想象中的推导过程"><a href="#想象中的推导过程" class="headerlink" title="想象中的推导过程"></a>想象中的推导过程</h3><script type="math/tex; mode=display">Xw = Y</script><script type="math/tex; mode=display">X^TXw = X^TY</script><script type="math/tex; mode=display">w = (X^TX)^{-1}X^TY</script><p>步骤很简单，但是其实这是一种野路子的算法，因为这一步的原始方程$Xw = Y$不是一个能直接使用的方程，这个方程可以通过坐标投影得到，只能作为一个经验公式使用，具体可以参见<a href="https://zhuanlan.zhihu.com/p/22757336。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22757336。</a></p><h3 id="真实的推导过程"><a href="#真实的推导过程" class="headerlink" title="真实的推导过程"></a>真实的推导过程</h3><p>推导之前需要了解一些关于矩阵范数的知识。</p><h3 id="矩阵范数"><a href="#矩阵范数" class="headerlink" title="矩阵范数"></a>矩阵范数</h3><p>矩阵的L2范数</p><script type="math/tex; mode=display">||A||^2_2 = \lambda</script><p>其中$\lambda$是矩阵$A^TA$的最大特征值。</p><p>因此，对于一维矩阵（向量）$a$而言，其L2范数就是$a^Ta$，因为$a^Ta$是一个标量，所以$a^Ta$的值就是其最大特征值。</p><h3 id="矩阵微分"><a href="#矩阵微分" class="headerlink" title="矩阵微分"></a>矩阵微分</h3><p>当$X$，$\beta$是一维矩阵（向量）时，</p><script type="math/tex; mode=display">\frac{\partial \beta^TX}{X} = \beta</script><script type="math/tex; mode=display">\frac{\partial X^TX}{\partial X} = X</script><script type="math/tex; mode=display">\frac{X^TAX}{\partial X} = (A + A^T)X</script><p><a href="https://blog.csdn.net/u010976453/article/details/54381248" target="_blank" rel="noopener">矩阵微分公式</a></p><p>回归的目标是最小化</p><script type="math/tex; mode=display">min(||Y-XW||^2_2)</script><h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><script type="math/tex; mode=display">F = ||Y-XW||^2_2 = (Y-XW)^T(Y-XW)</script><script type="math/tex; mode=display">F = (Y^T - W^TX^T)(Y-XW)</script><script type="math/tex; mode=display">F = Y^TY - Y^TXW - W^TX^TY + W^TX^TXW</script><p>对F进行求导得到：</p><script type="math/tex; mode=display">\frac{\partial F}{\partial W} = - Y^TX - Y^TX + (X^TX + X^TX)W</script><script type="math/tex; mode=display">\frac{\partial F}{\partial W} = -2 Y^TX + 2X^TXW</script><p>令$\frac{\partial F}{\partial W} = 0$，有：</p><script type="math/tex; mode=display">\frac{\partial F}{\partial W} = -2 Y^TX + 2X^TXW = 0</script><script type="math/tex; mode=display">X^TXW = Y^TX</script><script type="math/tex; mode=display">W = (X^TX)^{-1} Y^TX</script><h3 id="利用矩阵的迹的性质推导"><a href="#利用矩阵的迹的性质推导" class="headerlink" title="利用矩阵的迹的性质推导"></a>利用矩阵的迹的性质推导</h3><p>参见斯坦福大学cs229课程笔记<br>依赖公式：</p><script type="math/tex; mode=display">tr(a) = a</script><script type="math/tex; mode=display">trAB = trBA</script><script type="math/tex; mode=display">trA = trA^T</script><script type="math/tex; mode=display">\nabla_AtrAB = B^T</script><script type="math/tex; mode=display">\nabla_{A^T} trABA^TC = B^TA^TC^T + BA^TC</script><p>其中a是一个实数</p><p>也可以写成</p><script type="math/tex; mode=display">\frac{\partial(trAB)}{\partial A} = B^T</script><script type="math/tex; mode=display">\frac{\partial(trABA^TC)}{A^T} = B^TA^TC^T + BA^TC</script><p>但是这个推导过程用梯度形式更好书写，所以下面还是遵照原笔记中的梯度公式进行推导</p><p>推导过程：</p><script type="math/tex; mode=display">F = ||Y-XW||^2_2 = (Y-XW)^T(Y-XW)</script><p>直接求导</p><script type="math/tex; mode=display">\nabla_W F(W) = \nabla_W(Y-XW)^T(Y-XW)</script><script type="math/tex; mode=display">= \nabla_W(Y^TY - Y^TXW - W^TX^TY + W^TX^TXW)</script><p>因为括号中的所有项的计算结果是一个实数，所以：</p><script type="math/tex; mode=display">\nabla_W F(W) = \nabla_W tr(Y^TY - Y^TXW - W^TX^TY + W^TX^TXW)</script><p>因为$trA = trA^T$，可以化简为：</p><script type="math/tex; mode=display">\nabla_W F(W) = \nabla_W (Y^TY - 2trY^TXW + trW^TX^TXW)</script><p>再利用公式$\nabla_{A^T} trABA^TC = B^TA^TC^T + BA^TC$进行化简，令$A^T = W$，$B = B^T = X^TX$，$C = I$:</p><script type="math/tex; mode=display">\nabla_W F(W) = \nabla_W (Y^TY - 2trY^TXW + trABA^TC)</script><p>展开为：</p><script type="math/tex; mode=display">\nabla_W F(W) = \nabla_W (Y^TY - 2trY^TXW) + X^TXW + X^TXW</script><script type="math/tex; mode=display">\nabla_W F(W) = \nabla_W (-2trY^TXW) + 2X^TXW</script><p>另外根据公式$\nabla_AtrAB = B^T$以及公式$trAB = trBA$可以得到$\nabla_AtrBA = B^T$，所以上式可以继续化简为：</p><script type="math/tex; mode=display">\nabla_W F(W) = -2X^TY + 2X^TXW</script><p>令梯度为0，即可得到$minF(W)$所对应的$W$：</p><script type="math/tex; mode=display">2X^TY = 2X^TXW</script><script type="math/tex; mode=display">W = (X^TX)^{-1}X^TY</script><p>推导完毕</p><h2 id="岭回归的矩阵运算-1"><a href="#岭回归的矩阵运算-1" class="headerlink" title="岭回归的矩阵运算"></a>岭回归的矩阵运算</h2><p>岭回归的矩阵运算推导跟线性回归类似，只是多了一个范数求导的过程</p><p>目标：</p><script type="math/tex; mode=display">min(||Y-XW||^2_2 + \lambda||W||^2_2)</script><h4 id="计算过程-1"><a href="#计算过程-1" class="headerlink" title="计算过程"></a>计算过程</h4><script type="math/tex; mode=display">F = ||Y-XW||^2_2 + \lambda||W||^2_2 = (Y-XW)^T(Y-XW) + \lambda W^TW</script><script type="math/tex; mode=display">F = (Y^T - W^TX^T)(Y-XW) + \lambda W^TW</script><script type="math/tex; mode=display">F = Y^TY - Y^TXW - W^TX^TY + W^TX^TXW + \lambda W^TW</script><p>对F进行求导得到：</p><script type="math/tex; mode=display">\frac{\partial F}{\partial W} = - Y^TX - Y^TX + (X^TX + X^TX)W + \lambda W</script><script type="math/tex; mode=display">\frac{\partial F}{\partial W} = -2 Y^TX + 2X^TXW + \lambda W</script><p>令$\frac{\partial F}{\partial W} = 0$，有：</p><script type="math/tex; mode=display">\frac{\partial F}{\partial W} = -2 Y^TX + 2X^TXW + \lambda W = 0</script><script type="math/tex; mode=display">(X^TX + \frac{\lambda I}{2})W = Y^TX</script><script type="math/tex; mode=display">W = (X^TX + \frac{\lambda I}{2})^{-1}Y^TX</script><p><a href="https://blog.csdn.net/computerme/article/details/50486937" target="_blank" rel="noopener">推导地址</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;岭回归的矩阵运算&quot;&gt;&lt;a href=&quot;#岭回归的矩阵运算&quot; class=&quot;headerlink&quot; title=&quot;岭回归的矩阵运算&quot;&gt;&lt;/a&gt;岭回归的矩阵运算&lt;/h1&gt;&lt;h2 id=&quot;普通线性回归的矩阵运算&quot;&gt;&lt;a href=&quot;#普通线性回归的矩阵运算&quot; class
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="http://yoursite.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
</feed>
